{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-04T06:05:43.403606Z",
     "start_time": "2024-11-04T06:05:28.699372Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from torchvision.datasets import MNIST\n",
    "import time\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def download_mnist(is_train: bool):\n",
    "    dataset = MNIST(root='./data',\n",
    "                   transform=lambda x: np.array(x).flatten() / 255.0,\n",
    "                   download=True,\n",
    "                   train=is_train)\n",
    "    mnist_data = []\n",
    "    mnist_labels = []\n",
    "    for image, label in dataset:\n",
    "        mnist_data.append(image)\n",
    "        mnist_labels.append(label)\n",
    "    return np.array(mnist_data), np.array(mnist_labels)\n",
    "\n",
    "# Download and prepare the data\n",
    "train_X, train_Y = download_mnist(True)\n"
   ],
   "id": "c3bacfc7362ab654",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-04T06:05:44.432485Z",
     "start_time": "2024-11-04T06:05:43.411729Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "test_X, test_Y = download_mnist(False)\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "def create_one_hot(labels, num_classes=10):\n",
    "    one_hot = np.zeros((len(labels), num_classes))\n",
    "    one_hot[np.arange(len(labels)), labels] = 1\n",
    "    return one_hot\n",
    "\n",
    "train_Y = create_one_hot(train_Y)\n",
    "test_Y = create_one_hot(test_Y)"
   ],
   "id": "24c2bade212d4b58",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-04T06:26:16.603808Z",
     "start_time": "2024-11-04T06:25:13.545184Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "class MLPClassifier:\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size: int = 784,\n",
    "        hidden_size: int = 100,\n",
    "        output_size: int = 10,\n",
    "        learning_rate: float = 0.1,\n",
    "        batch_size: int = 128\n",
    "    ):\n",
    "        # Initialize weights using He initialization\n",
    "        self.W1 = np.random.randn(input_size, hidden_size) * np.sqrt(2.0/input_size)\n",
    "        self.b1 = np.zeros(hidden_size)\n",
    "        self.W2 = np.random.randn(hidden_size, output_size) * np.sqrt(2.0/hidden_size)\n",
    "        self.b2 = np.zeros(output_size)\n",
    "        \n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def relu(self, x):\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    def relu_derivative(self, x):\n",
    "        return np.where(x > 0, 1, 0)\n",
    "\n",
    "    def softmax(self, x):\n",
    "        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "        return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "\n",
    "    def forward(self, X, training: bool = True):\n",
    "        # First layer\n",
    "        Z1 = np.dot(X, self.W1) + self.b1\n",
    "        A1 = self.relu(Z1)\n",
    "        \n",
    "        # Output layer\n",
    "        R2 = np.dot(A1, self.W2) + self.b2\n",
    "        A2 = self.softmax(R2)\n",
    "        \n",
    "        return Z1, A1, A2\n",
    "\n",
    "    def backward(self, X, y, Z1, A1, A2):\n",
    "        m = X.shape[0]\n",
    "        \n",
    "        # Output layer gradients\n",
    "        dZ2 = A2 - y # derivative of loss with respect to pre-activation\n",
    "        dW2 = (1/m) * np.dot(A1.T, dZ2)\n",
    "        db2 = (1/m) * np.sum(dZ2, axis=0)\n",
    "        \n",
    "        # Hidden layer gradients\n",
    "        dA1 = np.dot(dZ2, self.W2.T)\n",
    "        dZ1 = dA1 * self.relu_derivative(Z1)\n",
    "        dW1 = (1/m) * np.dot(X.T, dZ1)\n",
    "        db1 = (1/m) * np.sum(dZ1, axis=0)\n",
    "        \n",
    "        # Update weights with L2 regularization\n",
    "        lambda_reg = 0.0001  # L2 regularization parameter\n",
    "        self.W2 -= self.learning_rate * (dW2 + lambda_reg * self.W2) # add the gradient of the L2\n",
    "        self.b2 -= self.learning_rate * db2\n",
    "        self.W1 -= self.learning_rate * (dW1 + lambda_reg * self.W1)\n",
    "        self.b1 -= self.learning_rate * db1\n",
    "\n",
    "    def train(self, X_train, y_train, X_val, y_val, epochs: int = 10):\n",
    "        start_time = time.time()\n",
    "        history = []\n",
    "        \n",
    "        n_samples = X_train.shape[0]\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            \n",
    "            # Mini-batch training\n",
    "            for i in range(0, n_samples, self.batch_size):\n",
    "                batch_X = X_train[i:i + self.batch_size]\n",
    "                batch_y = y_train[i:i + self.batch_size]\n",
    "                \n",
    "                # Forward pass\n",
    "                Z1, A1, A2 = self.forward(batch_X, training=True)\n",
    "                \n",
    "                # Backward pass\n",
    "                self.backward(batch_X, batch_y, Z1, A1, A2)\n",
    "            \n",
    "            # Compute training metrics\n",
    "            _, _, train_predictions = self.forward(X_train, training=False)\n",
    "            train_accuracy = accuracy_score(np.argmax(y_train, axis=1), \n",
    "                                         np.argmax(train_predictions, axis=1))\n",
    "            \n",
    "            # Compute validation metrics\n",
    "            _, _, val_predictions = self.forward(X_val, training=False)\n",
    "            val_accuracy = accuracy_score(np.argmax(y_val, axis=1), \n",
    "                                       np.argmax(val_predictions, axis=1))\n",
    "            \n",
    "            history.append((train_accuracy, val_accuracy))\n",
    "            \n",
    "            print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "            print(f\"Training Accuracy: {train_accuracy:.4f}\")\n",
    "            print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "            print(f\"Time elapsed: {time.time() - start_time:.2f}s\")\n",
    "            if history[(-2) % len(history)][0] > train_accuracy and len(history > 50):\n",
    "                print(f\"No further improvement in train accuracy. Stopping training\")\n",
    "                break\n",
    "            print(\"-\" * 50)\n",
    "            \n",
    "        return history\n",
    "\n",
    "    def predict(self, X):\n",
    "        _, _, predictions = self.forward(X, training=False)\n",
    "        return predictions\n",
    "\n",
    "\n",
    "# Initialize and train the model\n",
    "mlp = MLPClassifier(\n",
    "    input_size=784,\n",
    "    hidden_size=100,\n",
    "    output_size=10,\n",
    "    learning_rate=0.1,\n",
    "    batch_size=128\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "history = mlp.train(train_X, train_Y, test_X, test_Y, epochs=50)\n",
    "\n",
    "# Make predictions\n",
    "final_predictions = mlp.predict(test_X)\n",
    "final_accuracy = accuracy_score(np.argmax(test_Y, axis=1), \n",
    "                              np.argmax(final_predictions, axis=1))\n",
    "print(f\"Final Test Accuracy: {final_accuracy:.4f}\")"
   ],
   "id": "d1ec06d17fa7f617",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "Training Accuracy: 0.9131\n",
      "Validation Accuracy: 0.9169\n",
      "Time elapsed: 1.78s\n",
      "--------------------------------------------------\n",
      "Epoch 2/50\n",
      "Training Accuracy: 0.9326\n",
      "Validation Accuracy: 0.9331\n",
      "Time elapsed: 3.03s\n",
      "--------------------------------------------------\n",
      "Epoch 3/50\n",
      "Training Accuracy: 0.9438\n",
      "Validation Accuracy: 0.9408\n",
      "Time elapsed: 4.43s\n",
      "--------------------------------------------------\n",
      "Epoch 4/50\n",
      "Training Accuracy: 0.9519\n",
      "Validation Accuracy: 0.9479\n",
      "Time elapsed: 5.70s\n",
      "--------------------------------------------------\n",
      "Epoch 5/50\n",
      "Training Accuracy: 0.9580\n",
      "Validation Accuracy: 0.9533\n",
      "Time elapsed: 7.07s\n",
      "--------------------------------------------------\n",
      "Epoch 6/50\n",
      "Training Accuracy: 0.9627\n",
      "Validation Accuracy: 0.9581\n",
      "Time elapsed: 8.28s\n",
      "--------------------------------------------------\n",
      "Epoch 7/50\n",
      "Training Accuracy: 0.9666\n",
      "Validation Accuracy: 0.9616\n",
      "Time elapsed: 9.56s\n",
      "--------------------------------------------------\n",
      "Epoch 8/50\n",
      "Training Accuracy: 0.9697\n",
      "Validation Accuracy: 0.9645\n",
      "Time elapsed: 10.81s\n",
      "--------------------------------------------------\n",
      "Epoch 9/50\n",
      "Training Accuracy: 0.9720\n",
      "Validation Accuracy: 0.9670\n",
      "Time elapsed: 12.04s\n",
      "--------------------------------------------------\n",
      "Epoch 10/50\n",
      "Training Accuracy: 0.9739\n",
      "Validation Accuracy: 0.9687\n",
      "Time elapsed: 13.28s\n",
      "--------------------------------------------------\n",
      "Epoch 11/50\n",
      "Training Accuracy: 0.9757\n",
      "Validation Accuracy: 0.9700\n",
      "Time elapsed: 14.55s\n",
      "--------------------------------------------------\n",
      "Epoch 12/50\n",
      "Training Accuracy: 0.9774\n",
      "Validation Accuracy: 0.9714\n",
      "Time elapsed: 15.72s\n",
      "--------------------------------------------------\n",
      "Epoch 13/50\n",
      "Training Accuracy: 0.9786\n",
      "Validation Accuracy: 0.9723\n",
      "Time elapsed: 16.99s\n",
      "--------------------------------------------------\n",
      "Epoch 14/50\n",
      "Training Accuracy: 0.9796\n",
      "Validation Accuracy: 0.9734\n",
      "Time elapsed: 18.20s\n",
      "--------------------------------------------------\n",
      "Epoch 15/50\n",
      "Training Accuracy: 0.9806\n",
      "Validation Accuracy: 0.9737\n",
      "Time elapsed: 19.53s\n",
      "--------------------------------------------------\n",
      "Epoch 16/50\n",
      "Training Accuracy: 0.9818\n",
      "Validation Accuracy: 0.9745\n",
      "Time elapsed: 20.72s\n",
      "--------------------------------------------------\n",
      "Epoch 17/50\n",
      "Training Accuracy: 0.9829\n",
      "Validation Accuracy: 0.9748\n",
      "Time elapsed: 21.99s\n",
      "--------------------------------------------------\n",
      "Epoch 18/50\n",
      "Training Accuracy: 0.9834\n",
      "Validation Accuracy: 0.9750\n",
      "Time elapsed: 23.22s\n",
      "--------------------------------------------------\n",
      "Epoch 19/50\n",
      "Training Accuracy: 0.9841\n",
      "Validation Accuracy: 0.9752\n",
      "Time elapsed: 24.50s\n",
      "--------------------------------------------------\n",
      "Epoch 20/50\n",
      "Training Accuracy: 0.9848\n",
      "Validation Accuracy: 0.9758\n",
      "Time elapsed: 25.77s\n",
      "--------------------------------------------------\n",
      "Epoch 21/50\n",
      "Training Accuracy: 0.9853\n",
      "Validation Accuracy: 0.9762\n",
      "Time elapsed: 27.02s\n",
      "--------------------------------------------------\n",
      "Epoch 22/50\n",
      "Training Accuracy: 0.9861\n",
      "Validation Accuracy: 0.9766\n",
      "Time elapsed: 28.24s\n",
      "--------------------------------------------------\n",
      "Epoch 23/50\n",
      "Training Accuracy: 0.9867\n",
      "Validation Accuracy: 0.9768\n",
      "Time elapsed: 29.53s\n",
      "--------------------------------------------------\n",
      "Epoch 24/50\n",
      "Training Accuracy: 0.9874\n",
      "Validation Accuracy: 0.9771\n",
      "Time elapsed: 30.84s\n",
      "--------------------------------------------------\n",
      "Epoch 25/50\n",
      "Training Accuracy: 0.9881\n",
      "Validation Accuracy: 0.9776\n",
      "Time elapsed: 31.99s\n",
      "--------------------------------------------------\n",
      "Epoch 26/50\n",
      "Training Accuracy: 0.9886\n",
      "Validation Accuracy: 0.9777\n",
      "Time elapsed: 33.21s\n",
      "--------------------------------------------------\n",
      "Epoch 27/50\n",
      "Training Accuracy: 0.9890\n",
      "Validation Accuracy: 0.9781\n",
      "Time elapsed: 34.49s\n",
      "--------------------------------------------------\n",
      "Epoch 28/50\n",
      "Training Accuracy: 0.9895\n",
      "Validation Accuracy: 0.9785\n",
      "Time elapsed: 35.67s\n",
      "--------------------------------------------------\n",
      "Epoch 29/50\n",
      "Training Accuracy: 0.9898\n",
      "Validation Accuracy: 0.9787\n",
      "Time elapsed: 36.99s\n",
      "--------------------------------------------------\n",
      "Epoch 30/50\n",
      "Training Accuracy: 0.9902\n",
      "Validation Accuracy: 0.9783\n",
      "Time elapsed: 38.20s\n",
      "--------------------------------------------------\n",
      "Epoch 31/50\n",
      "Training Accuracy: 0.9907\n",
      "Validation Accuracy: 0.9784\n",
      "Time elapsed: 39.50s\n",
      "--------------------------------------------------\n",
      "Epoch 32/50\n",
      "Training Accuracy: 0.9911\n",
      "Validation Accuracy: 0.9785\n",
      "Time elapsed: 40.71s\n",
      "--------------------------------------------------\n",
      "Epoch 33/50\n",
      "Training Accuracy: 0.9916\n",
      "Validation Accuracy: 0.9785\n",
      "Time elapsed: 41.97s\n",
      "--------------------------------------------------\n",
      "Epoch 34/50\n",
      "Training Accuracy: 0.9920\n",
      "Validation Accuracy: 0.9785\n",
      "Time elapsed: 43.20s\n",
      "--------------------------------------------------\n",
      "Epoch 35/50\n",
      "Training Accuracy: 0.9922\n",
      "Validation Accuracy: 0.9786\n",
      "Time elapsed: 44.49s\n",
      "--------------------------------------------------\n",
      "Epoch 36/50\n",
      "Training Accuracy: 0.9927\n",
      "Validation Accuracy: 0.9787\n",
      "Time elapsed: 45.72s\n",
      "--------------------------------------------------\n",
      "Epoch 37/50\n",
      "Training Accuracy: 0.9929\n",
      "Validation Accuracy: 0.9787\n",
      "Time elapsed: 46.97s\n",
      "--------------------------------------------------\n",
      "Epoch 38/50\n",
      "Training Accuracy: 0.9932\n",
      "Validation Accuracy: 0.9787\n",
      "Time elapsed: 48.21s\n",
      "--------------------------------------------------\n",
      "Epoch 39/50\n",
      "Training Accuracy: 0.9935\n",
      "Validation Accuracy: 0.9788\n",
      "Time elapsed: 49.41s\n",
      "--------------------------------------------------\n",
      "Epoch 40/50\n",
      "Training Accuracy: 0.9936\n",
      "Validation Accuracy: 0.9785\n",
      "Time elapsed: 50.66s\n",
      "--------------------------------------------------\n",
      "Epoch 41/50\n",
      "Training Accuracy: 0.9937\n",
      "Validation Accuracy: 0.9787\n",
      "Time elapsed: 51.91s\n",
      "--------------------------------------------------\n",
      "Epoch 42/50\n",
      "Training Accuracy: 0.9940\n",
      "Validation Accuracy: 0.9785\n",
      "Time elapsed: 53.14s\n",
      "--------------------------------------------------\n",
      "Epoch 43/50\n",
      "Training Accuracy: 0.9941\n",
      "Validation Accuracy: 0.9788\n",
      "Time elapsed: 54.32s\n",
      "--------------------------------------------------\n",
      "Epoch 44/50\n",
      "Training Accuracy: 0.9942\n",
      "Validation Accuracy: 0.9789\n",
      "Time elapsed: 55.56s\n",
      "--------------------------------------------------\n",
      "Epoch 45/50\n",
      "Training Accuracy: 0.9944\n",
      "Validation Accuracy: 0.9786\n",
      "Time elapsed: 56.75s\n",
      "--------------------------------------------------\n",
      "Epoch 46/50\n",
      "Training Accuracy: 0.9947\n",
      "Validation Accuracy: 0.9789\n",
      "Time elapsed: 57.98s\n",
      "--------------------------------------------------\n",
      "Epoch 47/50\n",
      "Training Accuracy: 0.9949\n",
      "Validation Accuracy: 0.9791\n",
      "Time elapsed: 59.21s\n",
      "--------------------------------------------------\n",
      "Epoch 48/50\n",
      "Training Accuracy: 0.9952\n",
      "Validation Accuracy: 0.9791\n",
      "Time elapsed: 60.46s\n",
      "--------------------------------------------------\n",
      "Epoch 49/50\n",
      "Training Accuracy: 0.9953\n",
      "Validation Accuracy: 0.9790\n",
      "Time elapsed: 61.71s\n",
      "--------------------------------------------------\n",
      "Epoch 50/50\n",
      "Training Accuracy: 0.9955\n",
      "Validation Accuracy: 0.9788\n",
      "Time elapsed: 62.99s\n",
      "--------------------------------------------------\n",
      "Final Test Accuracy: 0.9788\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "$$\n",
    "L = -\\sum_i y_i \\log(a_i)\n",
    "$$\n",
    "$$\n",
    "L = -\\sum_i y_i \\log \\left( \\frac{e^{z_i}}{\\sum_j e^{z_j}} \\right)\n",
    "$$\n",
    "$$\n",
    "L = -\\sum_i y_i \\left( \\log(e^{z_i}) - \\log \\left( \\sum_j e^{z_j} \\right) \\right)\n",
    "$$\n",
    "$$\n",
    "L = -\\sum_i y_i \\left( z_i - \\log \\sum_j e^{z_j} \\right)\n",
    "$$\n",
    "$$\n",
    "L = -\\sum_i y_i z_i + \\sum_i y_i \\log \\sum_j e^{z_j}\n",
    "$$"
   ],
   "id": "bc625454873d8446"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "### The derivative of \\( L \\) with respect to \\( Z2 ) can be derived as follows:\n",
    "$$\n",
    "\\frac{\\partial}{\\partial z_k} \\left(-\\sum_i y_i z_i + \\sum_i y_i \\log \\sum_j e^{z_j} \\right) = -y_k + \\sum_i y_i \\cdot \\frac{\\partial}{\\partial z_k} \\log \\sum_j e^{z_j}\n",
    "$$\n",
    "\n",
    "Use the chain rule, we get:\n",
    "\n",
    "$$\n",
    "= -y_k + \\sum_i y_i \\cdot \\frac{1}{\\sum_j e^{z_j}} \\cdot e^{z_k} = -y_k + \\sum_i y_i \\cdot a_k\n",
    "$$\n",
    "\n",
    "For one-hot encoded labels, this simplifies to:\n",
    "\n",
    "$$\n",
    "= a_k - y_k\n",
    "$$"
   ],
   "id": "f653247829933ee6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "bea0687c7fb3fde9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### The derivative of \\( Z2 \\) with respect to \\( W2 \\) can be derived as follows:\n",
    "$$\n",
    "Z2 = A1 \\cdot W2 + b2 \n",
    "$$\n",
    "\n",
    "$$\n",
    " \\frac{\\partial Z2}{\\partial W2} = A1 \n",
    "$$\n",
    "\n",
    "The derivative of the loss with respect to pre-activation is:\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial Z2} = a_k - y_k \n",
    "$$\n",
    "\n",
    "Using the chain rule, the derivative of the loss with respect to \\( W2 \\) is:\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W2} = \\frac{\\partial Z2}{\\partial W2} \\cdot \\frac{\\partial L}{\\partial Z2}\n",
    "$$\n",
    "\n",
    "Therefore:\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W2} = A1^T \\cdot dZ2 \n",
    "$$"
   ],
   "id": "31326a66b9a488c1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### The derivative of the cost function \\( L \\) with respect to \\( A1 \\) can be derived as follows:\n",
    "\n",
    "The derivative of the loss with respect to \\( Z2 \\) (pre-activation) is:\n",
    "$$ \n",
    "\\frac{\\partial L}{\\partial Z2} = a - y \n",
    "$$\n",
    "\n",
    "Using the chain rule, the derivative of the loss with respect to \\( A1 \\) is:\n",
    "$$ \n",
    "\\frac{\\partial L}{\\partial A1} = \\frac{\\partial L}{\\partial Z2} \\cdot \\frac{\\partial Z2}{\\partial A1}\n",
    "$$\n",
    "\n",
    "$$ \n",
    "Z2 = A1 \\cdot W2 + b2 \n",
    "$$\n",
    "$$ \n",
    "\\frac{\\partial Z2}{\\partial A1} = W2 \n",
    "$$\n",
    "\n",
    "Therefore:\n",
    "$$ \\frac{\\partial L}{\\partial A1} = \\frac{\\partial L}{\\partial Z2} \\cdot W2^T $$"
   ],
   "id": "98dbb04ee0701f4b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
