{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-26T15:09:32.702974Z",
     "start_time": "2024-10-26T15:09:18.103355Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from torchvision.datasets import MNIST\n",
    "import time\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def download_mnist(is_train: bool):\n",
    "    dataset = MNIST(root='./data',\n",
    "                   transform=lambda x: np.array(x).flatten() / 255.0,\n",
    "                   download=True,\n",
    "                   train=is_train)\n",
    "    mnist_data = []\n",
    "    mnist_labels = []\n",
    "    for image, label in dataset:\n",
    "        mnist_data.append(image)\n",
    "        mnist_labels.append(label)\n",
    "    return np.array(mnist_data), np.array(mnist_labels)\n",
    "\n",
    "# Download and prepare the data\n",
    "train_X, train_Y = download_mnist(True)\n"
   ],
   "id": "c3bacfc7362ab654",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-26T15:09:33.793832Z",
     "start_time": "2024-10-26T15:09:32.740832Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "test_X, test_Y = download_mnist(False)\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "def create_one_hot(labels, num_classes=10):\n",
    "    one_hot = np.zeros((len(labels), num_classes))\n",
    "    one_hot[np.arange(len(labels)), labels] = 1\n",
    "    return one_hot\n",
    "\n",
    "train_Y = create_one_hot(train_Y)\n",
    "test_Y = create_one_hot(test_Y)"
   ],
   "id": "24c2bade212d4b58",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-26T17:53:38.501532Z",
     "start_time": "2024-10-26T17:53:10.794420Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "class MLPClassifier:\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size: int = 784,\n",
    "        hidden_size: int = 100,\n",
    "        output_size: int = 10,\n",
    "        learning_rate: float = 0.1,\n",
    "        batch_size: int = 128\n",
    "    ):\n",
    "        # Initialize weights using He initialization\n",
    "        self.W1 = np.random.randn(input_size, hidden_size) * np.sqrt(2.0/input_size)\n",
    "        self.b1 = np.zeros(hidden_size)\n",
    "        self.W2 = np.random.randn(hidden_size, output_size) * np.sqrt(2.0/hidden_size)\n",
    "        self.b2 = np.zeros(output_size)\n",
    "        \n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def relu(self, x: np.ndarray) -> np.ndarray:\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    def relu_derivative(self, x: np.ndarray) -> np.ndarray:\n",
    "        return np.where(x > 0, 1, 0)\n",
    "\n",
    "    def softmax(self, x: np.ndarray) -> np.ndarray:\n",
    "        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "        return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "\n",
    "    def forward(self, X: np.ndarray, training: bool = True) -> tuple:\n",
    "        # First layer\n",
    "        R1 = np.dot(X, self.W1) + self.b1\n",
    "        A1 = self.relu(R1)\n",
    "        \n",
    "        # Output layer\n",
    "        R2 = np.dot(A1, self.W2) + self.b2\n",
    "        A2 = self.softmax(R2)\n",
    "        \n",
    "        return R1, A1, A2\n",
    "\n",
    "    def backward(self, X: np.ndarray, y: np.ndarray, R1: np.ndarray, \n",
    "                A1: np.ndarray, A2: np.ndarray) -> None:\n",
    "        m = X.shape[0]\n",
    "        \n",
    "        # Output layer gradients\n",
    "        dR2 = A2 - y # derivative of loss with respect to pre-activation\n",
    "        dW2 = (1/m) * np.dot(A1.T, dR2)\n",
    "        db2 = (1/m) * np.sum(dR2, axis=0)\n",
    "        \n",
    "        # Hidden layer gradients\n",
    "        dA1 = np.dot(dR2, self.W2.T)\n",
    "        dR1 = dA1 * self.relu_derivative(R1)\n",
    "        dW1 = (1/m) * np.dot(X.T, dR1)\n",
    "        db1 = (1/m) * np.sum(dR1, axis=0)\n",
    "        \n",
    "        # Update weights with L2 regularization\n",
    "        lambda_reg = 0.0001  # L2 regularization parameter\n",
    "        self.W2 -= self.learning_rate * (dW2 + lambda_reg * self.W2) # add the gradient of the L2\n",
    "        self.b2 -= self.learning_rate * db2\n",
    "        self.W1 -= self.learning_rate * (dW1 + lambda_reg * self.W1)\n",
    "        self.b1 -= self.learning_rate * db1\n",
    "\n",
    "    def train(self, X_train: np.ndarray, y_train: np.ndarray, \n",
    "              X_val: np.ndarray, y_val: np.ndarray, epochs: int = 10) -> list:\n",
    "        start_time = time.time()\n",
    "        history = []\n",
    "        \n",
    "        # Convert inputs to numpy arrays if they aren't already\n",
    "        X_train = np.array(X_train)\n",
    "        y_train = np.array(y_train)\n",
    "        X_val = np.array(X_val)\n",
    "        y_val = np.array(y_val)\n",
    "        \n",
    "        n_samples = X_train.shape[0]\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Shuffle training data\n",
    "            shuffle_idx = np.random.permutation(n_samples)\n",
    "            X_train = X_train[shuffle_idx]\n",
    "            y_train = y_train[shuffle_idx]\n",
    "            \n",
    "            # Mini-batch training\n",
    "            for i in range(0, n_samples, self.batch_size):\n",
    "                batch_X = X_train[i:i + self.batch_size]\n",
    "                batch_y = y_train[i:i + self.batch_size]\n",
    "                \n",
    "                # Forward pass\n",
    "                R1, A1, A2 = self.forward(batch_X, training=True)\n",
    "                \n",
    "                # Backward pass\n",
    "                self.backward(batch_X, batch_y, R1, A1, A2)\n",
    "            \n",
    "            # Compute training metrics\n",
    "            _, _, train_predictions = self.forward(X_train, training=False)\n",
    "            train_accuracy = accuracy_score(np.argmax(y_train, axis=1), \n",
    "                                         np.argmax(train_predictions, axis=1))\n",
    "            \n",
    "            # Compute validation metrics\n",
    "            _, _, val_predictions = self.forward(X_val, training=False)\n",
    "            val_accuracy = accuracy_score(np.argmax(y_val, axis=1), \n",
    "                                       np.argmax(val_predictions, axis=1))\n",
    "            \n",
    "            history.append((train_accuracy, val_accuracy))\n",
    "            \n",
    "            print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "            print(f\"Training Accuracy: {train_accuracy:.4f}\")\n",
    "            print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "            print(f\"Time elapsed: {time.time() - start_time:.2f}s\")\n",
    "            print(\"-\" * 50)\n",
    "            \n",
    "        return history\n",
    "\n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        _, _, _, predictions = self.forward(X, training=False)\n",
    "        return predictions\n",
    "\n",
    "\n",
    "# Initialize and train the model\n",
    "mlp = MLPClassifier(\n",
    "    input_size=784,\n",
    "    hidden_size=100,\n",
    "    output_size=10,\n",
    "    learning_rate=0.1,\n",
    "    batch_size=128\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "history = mlp.train(train_X, train_Y, test_X, test_Y, epochs=100)\n",
    "\n",
    "# Make predictions\n",
    "final_predictions = mlp.predict(test_X)\n",
    "final_accuracy = accuracy_score(np.argmax(test_Y, axis=1), \n",
    "                              np.argmax(final_predictions, axis=1))\n",
    "print(f\"Final Test Accuracy: {final_accuracy:.4f}\")"
   ],
   "id": "d1ec06d17fa7f617",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "Training Accuracy: 0.9172\n",
      "Validation Accuracy: 0.9205\n",
      "Time elapsed: 1.72s\n",
      "--------------------------------------------------\n",
      "Epoch 2/100\n",
      "Training Accuracy: 0.9344\n",
      "Validation Accuracy: 0.9344\n",
      "Time elapsed: 3.19s\n",
      "--------------------------------------------------\n",
      "Epoch 3/100\n",
      "Training Accuracy: 0.9457\n",
      "Validation Accuracy: 0.9443\n",
      "Time elapsed: 4.77s\n",
      "--------------------------------------------------\n",
      "Epoch 4/100\n",
      "Training Accuracy: 0.9527\n",
      "Validation Accuracy: 0.9501\n",
      "Time elapsed: 6.28s\n",
      "--------------------------------------------------\n",
      "Epoch 5/100\n",
      "Training Accuracy: 0.9588\n",
      "Validation Accuracy: 0.9547\n",
      "Time elapsed: 7.91s\n",
      "--------------------------------------------------\n",
      "Epoch 6/100\n",
      "Training Accuracy: 0.9631\n",
      "Validation Accuracy: 0.9596\n",
      "Time elapsed: 9.34s\n",
      "--------------------------------------------------\n",
      "Epoch 7/100\n",
      "Training Accuracy: 0.9680\n",
      "Validation Accuracy: 0.9629\n",
      "Time elapsed: 10.84s\n",
      "--------------------------------------------------\n",
      "Epoch 8/100\n",
      "Training Accuracy: 0.9702\n",
      "Validation Accuracy: 0.9657\n",
      "Time elapsed: 12.54s\n",
      "--------------------------------------------------\n",
      "Epoch 9/100\n",
      "Training Accuracy: 0.9727\n",
      "Validation Accuracy: 0.9659\n",
      "Time elapsed: 14.21s\n",
      "--------------------------------------------------\n",
      "Epoch 10/100\n",
      "Training Accuracy: 0.9748\n",
      "Validation Accuracy: 0.9681\n",
      "Time elapsed: 15.82s\n",
      "--------------------------------------------------\n",
      "Epoch 11/100\n",
      "Training Accuracy: 0.9770\n",
      "Validation Accuracy: 0.9697\n",
      "Time elapsed: 17.34s\n",
      "--------------------------------------------------\n",
      "Epoch 12/100\n",
      "Training Accuracy: 0.9776\n",
      "Validation Accuracy: 0.9701\n",
      "Time elapsed: 18.90s\n",
      "--------------------------------------------------\n",
      "Epoch 13/100\n",
      "Training Accuracy: 0.9798\n",
      "Validation Accuracy: 0.9718\n",
      "Time elapsed: 20.48s\n",
      "--------------------------------------------------\n",
      "Epoch 14/100\n",
      "Training Accuracy: 0.9808\n",
      "Validation Accuracy: 0.9718\n",
      "Time elapsed: 22.02s\n",
      "--------------------------------------------------\n",
      "Epoch 15/100\n",
      "Training Accuracy: 0.9813\n",
      "Validation Accuracy: 0.9715\n",
      "Time elapsed: 23.60s\n",
      "--------------------------------------------------\n",
      "Epoch 16/100\n",
      "Training Accuracy: 0.9833\n",
      "Validation Accuracy: 0.9740\n",
      "Time elapsed: 25.22s\n",
      "--------------------------------------------------\n",
      "Epoch 17/100\n",
      "Training Accuracy: 0.9836\n",
      "Validation Accuracy: 0.9735\n",
      "Time elapsed: 26.81s\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[10], line 127\u001B[0m\n\u001B[0;32m    118\u001B[0m mlp \u001B[38;5;241m=\u001B[39m MLPClassifier(\n\u001B[0;32m    119\u001B[0m     input_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m784\u001B[39m,\n\u001B[0;32m    120\u001B[0m     hidden_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m100\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    123\u001B[0m     batch_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m128\u001B[39m\n\u001B[0;32m    124\u001B[0m )\n\u001B[0;32m    126\u001B[0m \u001B[38;5;66;03m# Train the model\u001B[39;00m\n\u001B[1;32m--> 127\u001B[0m history \u001B[38;5;241m=\u001B[39m \u001B[43mmlp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_X\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_Y\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_X\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_Y\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m100\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m    129\u001B[0m \u001B[38;5;66;03m# Make predictions\u001B[39;00m\n\u001B[0;32m    130\u001B[0m final_predictions \u001B[38;5;241m=\u001B[39m mlp\u001B[38;5;241m.\u001B[39mpredict(test_X)\n",
      "Cell \u001B[1;32mIn[10], line 87\u001B[0m, in \u001B[0;36mMLPClassifier.train\u001B[1;34m(self, X_train, y_train, X_val, y_val, epochs)\u001B[0m\n\u001B[0;32m     84\u001B[0m batch_y \u001B[38;5;241m=\u001B[39m y_train[i:i \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbatch_size]\n\u001B[0;32m     86\u001B[0m \u001B[38;5;66;03m# Forward pass\u001B[39;00m\n\u001B[1;32m---> 87\u001B[0m R1, A1, A2 \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mforward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch_X\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtraining\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m     89\u001B[0m \u001B[38;5;66;03m# Backward pass\u001B[39;00m\n\u001B[0;32m     90\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbackward(batch_X, batch_y, R1, A1, A2)\n",
      "Cell \u001B[1;32mIn[10], line 31\u001B[0m, in \u001B[0;36mMLPClassifier.forward\u001B[1;34m(self, X, training)\u001B[0m\n\u001B[0;32m     29\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, X: np\u001B[38;5;241m.\u001B[39mndarray, training: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mtuple\u001B[39m:\n\u001B[0;32m     30\u001B[0m     \u001B[38;5;66;03m# First layer\u001B[39;00m\n\u001B[1;32m---> 31\u001B[0m     R1 \u001B[38;5;241m=\u001B[39m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdot\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mW1\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mb1\n\u001B[0;32m     32\u001B[0m     A1 \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrelu(R1)\n\u001B[0;32m     34\u001B[0m     \u001B[38;5;66;03m# Output layer\u001B[39;00m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "∂L/∂z_k = -∑(y_i/a_i * a_i(δik - a_k))\n",
    "        = -∑(y_i(δik - a_k))\n",
    "        = -y_k + a_k∑y_i\n",
    "        = a_k - y_k  (since ∑y_i = 1 for one-hot encoded labels)"
   ],
   "id": "297e0888cf6c90c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
