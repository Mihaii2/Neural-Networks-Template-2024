{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-03T18:47:10.051247Z",
     "start_time": "2024-11-03T18:46:55.595553Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from torchvision.datasets import MNIST\n",
    "import time\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def download_mnist(is_train: bool):\n",
    "    dataset = MNIST(root='./data',\n",
    "                   transform=lambda x: np.array(x).flatten() / 255.0,\n",
    "                   download=True,\n",
    "                   train=is_train)\n",
    "    mnist_data = []\n",
    "    mnist_labels = []\n",
    "    for image, label in dataset:\n",
    "        mnist_data.append(image)\n",
    "        mnist_labels.append(label)\n",
    "    return np.array(mnist_data), np.array(mnist_labels)\n",
    "\n",
    "# Download and prepare the data\n",
    "train_X, train_Y = download_mnist(True)\n"
   ],
   "id": "c3bacfc7362ab654",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-03T18:47:11.178103Z",
     "start_time": "2024-11-03T18:47:10.059263Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "test_X, test_Y = download_mnist(False)\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "def create_one_hot(labels, num_classes=10):\n",
    "    one_hot = np.zeros((len(labels), num_classes))\n",
    "    one_hot[np.arange(len(labels)), labels] = 1\n",
    "    return one_hot\n",
    "\n",
    "train_Y = create_one_hot(train_Y)\n",
    "test_Y = create_one_hot(test_Y)"
   ],
   "id": "24c2bade212d4b58",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-03T19:04:16.644911Z",
     "start_time": "2024-11-03T19:03:56.148656Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "class MLPClassifier:\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size: int = 784,\n",
    "        hidden_size: int = 100,\n",
    "        output_size: int = 10,\n",
    "        learning_rate: float = 0.1,\n",
    "        batch_size: int = 128\n",
    "    ):\n",
    "        # Initialize weights using He initialization\n",
    "        self.W1 = np.random.randn(input_size, hidden_size) * np.sqrt(2.0/input_size)\n",
    "        self.b1 = np.zeros(hidden_size)\n",
    "        self.W2 = np.random.randn(hidden_size, output_size) * np.sqrt(2.0/hidden_size)\n",
    "        self.b2 = np.zeros(output_size)\n",
    "        \n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def relu(self, x):\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    def relu_derivative(self, x):\n",
    "        return np.where(x > 0, 1, 0)\n",
    "\n",
    "    def softmax(self, x):\n",
    "        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "        return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "\n",
    "    def forward(self, X, training: bool = True):\n",
    "        # First layer\n",
    "        Z1 = np.dot(X, self.W1) + self.b1\n",
    "        A1 = self.relu(Z1)\n",
    "        \n",
    "        # Output layer\n",
    "        R2 = np.dot(A1, self.W2) + self.b2\n",
    "        A2 = self.softmax(R2)\n",
    "        \n",
    "        return Z1, A1, A2\n",
    "\n",
    "    def backward(self, X, y, Z1, A1, A2):\n",
    "        m = X.shape[0]\n",
    "        \n",
    "        # Output layer gradients\n",
    "        dZ2 = A2 - y # derivative of loss with respect to pre-activation\n",
    "        dW2 = (1/m) * np.dot(A1.T, dZ2)\n",
    "        db2 = (1/m) * np.sum(dZ2, axis=0)\n",
    "        \n",
    "        # Hidden layer gradients\n",
    "        dA1 = np.dot(dZ2, self.W2.T)\n",
    "        dZ1 = dA1 * self.relu_derivative(Z1)\n",
    "        dW1 = (1/m) * np.dot(X.T, dZ1)\n",
    "        db1 = (1/m) * np.sum(dZ1, axis=0)\n",
    "        \n",
    "        # Update weights with L2 regularization\n",
    "        lambda_reg = 0.0001  # L2 regularization parameter\n",
    "        self.W2 -= self.learning_rate * (dW2 + lambda_reg * self.W2) # add the gradient of the L2\n",
    "        self.b2 -= self.learning_rate * db2\n",
    "        self.W1 -= self.learning_rate * (dW1 + lambda_reg * self.W1)\n",
    "        self.b1 -= self.learning_rate * db1\n",
    "\n",
    "    def train(self, X_train, y_train, X_val, y_val, epochs: int = 10):\n",
    "        start_time = time.time()\n",
    "        history = []\n",
    "        \n",
    "        # Convert inputs to numpy arrays if they aren't already\n",
    "        X_train = np.array(X_train)\n",
    "        y_train = np.array(y_train)\n",
    "        X_val = np.array(X_val)\n",
    "        y_val = np.array(y_val)\n",
    "        \n",
    "        n_samples = X_train.shape[0]\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Shuffle training data\n",
    "            shuffle_idx = np.random.permutation(n_samples)\n",
    "            X_train = X_train[shuffle_idx]\n",
    "            y_train = y_train[shuffle_idx]\n",
    "            \n",
    "            # Mini-batch training\n",
    "            for i in range(0, n_samples, self.batch_size):\n",
    "                batch_X = X_train[i:i + self.batch_size]\n",
    "                batch_y = y_train[i:i + self.batch_size]\n",
    "                \n",
    "                # Forward pass\n",
    "                Z1, A1, A2 = self.forward(batch_X, training=True)\n",
    "                \n",
    "                # Backward pass\n",
    "                self.backward(batch_X, batch_y, Z1, A1, A2)\n",
    "            \n",
    "            # Compute training metrics\n",
    "            _, _, train_predictions = self.forward(X_train, training=False)\n",
    "            train_accuracy = accuracy_score(np.argmax(y_train, axis=1), \n",
    "                                         np.argmax(train_predictions, axis=1))\n",
    "            \n",
    "            # Compute validation metrics\n",
    "            _, _, val_predictions = self.forward(X_val, training=False)\n",
    "            val_accuracy = accuracy_score(np.argmax(y_val, axis=1), \n",
    "                                       np.argmax(val_predictions, axis=1))\n",
    "            \n",
    "            history.append((train_accuracy, val_accuracy))\n",
    "            \n",
    "            print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "            print(f\"Training Accuracy: {train_accuracy:.4f}\")\n",
    "            print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "            print(f\"Time elapsed: {time.time() - start_time:.2f}s\")\n",
    "            if history[(-2) % len(history)][0] > train_accuracy:\n",
    "                print(f\"No further improvement in train accuracy. Stopping training\")\n",
    "                break\n",
    "            print(\"-\" * 50)\n",
    "            \n",
    "        return history\n",
    "\n",
    "    def predict(self, X):\n",
    "        _, _, predictions = self.forward(X, training=False)\n",
    "        return predictions\n",
    "\n",
    "\n",
    "# Initialize and train the model\n",
    "mlp = MLPClassifier(\n",
    "    input_size=784,\n",
    "    hidden_size=100,\n",
    "    output_size=10,\n",
    "    learning_rate=0.1,\n",
    "    batch_size=128\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "history = mlp.train(train_X, train_Y, test_X, test_Y, epochs=100)\n",
    "\n",
    "# Make predictions\n",
    "final_predictions = mlp.predict(test_X)\n",
    "final_accuracy = accuracy_score(np.argmax(test_Y, axis=1), \n",
    "                              np.argmax(final_predictions, axis=1))\n",
    "print(f\"Final Test Accuracy: {final_accuracy:.4f}\")"
   ],
   "id": "d1ec06d17fa7f617",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "Training Accuracy: 0.9153\n",
      "Validation Accuracy: 0.9184\n",
      "Time elapsed: 2.03s\n",
      "--------------------------------------------------\n",
      "Epoch 2/100\n",
      "Training Accuracy: 0.9354\n",
      "Validation Accuracy: 0.9346\n",
      "Time elapsed: 3.53s\n",
      "--------------------------------------------------\n",
      "Epoch 3/100\n",
      "Training Accuracy: 0.9474\n",
      "Validation Accuracy: 0.9461\n",
      "Time elapsed: 5.15s\n",
      "--------------------------------------------------\n",
      "Epoch 4/100\n",
      "Training Accuracy: 0.9527\n",
      "Validation Accuracy: 0.9512\n",
      "Time elapsed: 6.75s\n",
      "--------------------------------------------------\n",
      "Epoch 5/100\n",
      "Training Accuracy: 0.9601\n",
      "Validation Accuracy: 0.9573\n",
      "Time elapsed: 8.35s\n",
      "--------------------------------------------------\n",
      "Epoch 6/100\n",
      "Training Accuracy: 0.9650\n",
      "Validation Accuracy: 0.9610\n",
      "Time elapsed: 9.88s\n",
      "--------------------------------------------------\n",
      "Epoch 7/100\n",
      "Training Accuracy: 0.9673\n",
      "Validation Accuracy: 0.9635\n",
      "Time elapsed: 11.54s\n",
      "--------------------------------------------------\n",
      "Epoch 8/100\n",
      "Training Accuracy: 0.9712\n",
      "Validation Accuracy: 0.9659\n",
      "Time elapsed: 13.06s\n",
      "--------------------------------------------------\n",
      "Epoch 9/100\n",
      "Training Accuracy: 0.9734\n",
      "Validation Accuracy: 0.9673\n",
      "Time elapsed: 14.56s\n",
      "--------------------------------------------------\n",
      "Epoch 10/100\n",
      "Training Accuracy: 0.9753\n",
      "Validation Accuracy: 0.9688\n",
      "Time elapsed: 15.99s\n",
      "--------------------------------------------------\n",
      "Epoch 11/100\n",
      "Training Accuracy: 0.9757\n",
      "Validation Accuracy: 0.9705\n",
      "Time elapsed: 17.57s\n",
      "--------------------------------------------------\n",
      "Epoch 12/100\n",
      "Training Accuracy: 0.9787\n",
      "Validation Accuracy: 0.9712\n",
      "Time elapsed: 18.96s\n",
      "--------------------------------------------------\n",
      "Epoch 13/100\n",
      "Training Accuracy: 0.9785\n",
      "Validation Accuracy: 0.9708\n",
      "Time elapsed: 20.38s\n",
      "No further improvement in train accuracy. Stopping training\n",
      "Final Test Accuracy: 0.9708\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "$$\n",
    "L = -\\sum_i y_i \\log(a_i)\n",
    "$$\n",
    "$$\n",
    "L = -\\sum_i y_i \\log \\left( \\frac{e^{z_i}}{\\sum_j e^{z_j}} \\right)\n",
    "$$\n",
    "$$\n",
    "L = -\\sum_i y_i \\left( \\log(e^{z_i}) - \\log \\left( \\sum_j e^{z_j} \\right) \\right)\n",
    "$$\n",
    "$$\n",
    "L = -\\sum_i y_i \\left( z_i - \\log \\sum_j e^{z_j} \\right)\n",
    "$$\n",
    "$$\n",
    "L = -\\sum_i y_i z_i + \\sum_i y_i \\log \\sum_j e^{z_j}\n",
    "$$"
   ],
   "id": "bc625454873d8446"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "### The derivative of \\( L \\) with respect to \\( Z2 ) can be derived as follows:\n",
    "$$\n",
    "\\frac{\\partial}{\\partial z_k} \\left(-\\sum_i y_i z_i + \\sum_i y_i \\log \\sum_j e^{z_j} \\right) = -y_k + \\sum_i y_i \\cdot \\frac{\\partial}{\\partial z_k} \\log \\sum_j e^{z_j}\n",
    "$$\n",
    "\n",
    "Use the chain rule, we get:\n",
    "\n",
    "$$\n",
    "= -y_k + \\sum_i y_i \\cdot \\frac{1}{\\sum_j e^{z_j}} \\cdot e^{z_k} = -y_k + \\sum_i y_i \\cdot a_k\n",
    "$$\n",
    "\n",
    "For one-hot encoded labels, this simplifies to:\n",
    "\n",
    "$$\n",
    "= a_k - y_k\n",
    "$$"
   ],
   "id": "f653247829933ee6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "bea0687c7fb3fde9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### The derivative of \\( Z2 \\) with respect to \\( W2 \\) can be derived as follows:\n",
    "$$\n",
    "Z2 = A1 \\cdot W2 + b2 \n",
    "$$\n",
    "\n",
    "$$\n",
    " \\frac{\\partial Z2}{\\partial W2} = A1 \n",
    "$$\n",
    "\n",
    "The derivative of the loss with respect to pre-activation is:\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial Z2} = a_k - y_k \n",
    "$$\n",
    "\n",
    "Using the chain rule, the derivative of the loss with respect to \\( W2 \\) is:\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W2} = \\frac{\\partial Z2}{\\partial W2} \\cdot \\frac{\\partial L}{\\partial Z2}\n",
    "$$\n",
    "\n",
    "Therefore:\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W2} = A1^T \\cdot dZ2 \n",
    "$$"
   ],
   "id": "31326a66b9a488c1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### The derivative of the cost function \\( L \\) with respect to \\( A1 \\) can be derived as follows:\n",
    "\n",
    "The derivative of the loss with respect to \\( Z2 \\) (pre-activation) is:\n",
    "$$ \n",
    "\\frac{\\partial L}{\\partial Z2} = a - y \n",
    "$$\n",
    "\n",
    "Using the chain rule, the derivative of the loss with respect to \\( A1 \\) is:\n",
    "$$ \n",
    "\\frac{\\partial L}{\\partial A1} = \\frac{\\partial L}{\\partial Z2} \\cdot \\frac{\\partial Z2}{\\partial A1}\n",
    "$$\n",
    "\n",
    "$$ \n",
    "Z2 = A1 \\cdot W2 + b2 \n",
    "$$\n",
    "$$ \n",
    "\\frac{\\partial Z2}{\\partial A1} = W2 \n",
    "$$\n",
    "\n",
    "Therefore:\n",
    "$$ \\frac{\\partial L}{\\partial A1} = \\frac{\\partial L}{\\partial Z2} \\cdot W2^T $$"
   ],
   "id": "98dbb04ee0701f4b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
