{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-12-24T14:08:17.169099Z"
    }
   },
   "source": [
    "import gymnasium as gym\n",
    "import flappy_bird_gymnasium\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import random\n",
    "from PIL import Image\n",
    "import os\n",
    "from datetime import datetime\n",
    "import yaml\n",
    "import gc\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_channels=4, n_actions=2):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, 32, kernel_size=8, stride=4),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.LeakyReLU(0.01)\n",
    "        )\n",
    "\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(3136, 512), # 3136 = 64 * 7 * 7\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.Linear(512, n_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc_layers(x)\n",
    "\n",
    "class FrameStack:\n",
    "    def __init__(self, size=4):\n",
    "        self.size = size\n",
    "        self.frames = deque(maxlen=size)\n",
    "    \n",
    "    def push(self, frame):\n",
    "        self.frames.append(frame)\n",
    "        # Randomly save visualizations\n",
    "        if random.random() < 0.01:\n",
    "            self.visualize_stack()\n",
    "    \n",
    "    def get_state(self):\n",
    "        # If we don't have enough frames, duplicate the last frame\n",
    "        while len(self.frames) < self.size:\n",
    "            if len(self.frames) > 0:\n",
    "                self.frames.append(self.frames[-1])\n",
    "            else:\n",
    "                # Create a zero tensor with same shape as expected frame: [1, 1, 84, 84]\n",
    "                zero_frame = torch.zeros(1, 1, 84, 84)\n",
    "                self.frames.append(zero_frame)\n",
    "        \n",
    "        # Stack frames along channel dimension\n",
    "        return torch.cat(list(self.frames), dim=1)  # Will give [1, 4, 84, 84]\n",
    "\n",
    "    def visualize_stack(self):\n",
    "        \"\"\"Save visualization of the current frame stack\"\"\"\n",
    "        if len(self.frames) == 0:\n",
    "            return\n",
    "            \n",
    "        os.makedirs('frame_stacks', exist_ok=True)\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        \n",
    "        # Create a figure with subplots for each frame\n",
    "        import matplotlib.pyplot as plt\n",
    "        fig, axs = plt.subplots(1, self.size, figsize=(20, 5))\n",
    "        \n",
    "        for i, frame in enumerate(self.frames):\n",
    "            # Convert tensor to numpy and remove batch and channel dimensions\n",
    "            frame_np = frame.squeeze().numpy()\n",
    "            axs[i].imshow(frame_np, cmap='gray')\n",
    "            axs[i].axis('off')\n",
    "            axs[i].set_title(f'Frame {i+1}')\n",
    "        \n",
    "        plt.suptitle('Current Frame Stack')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join('frame_stacks', f'frame_stack_{timestamp}.png'))\n",
    "        plt.close()\n",
    "\n",
    "    def clear(self):\n",
    "        self.frames.clear()\n",
    "\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity=50000): \n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        # Move tensors to CPU and detach from computation graph\n",
    "        if torch.is_tensor(state):\n",
    "            state = state.cpu().detach()\n",
    "        if torch.is_tensor(next_state):\n",
    "            next_state = next_state.cpu().detach()\n",
    "\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        samples = random.sample(self.buffer, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*samples)\n",
    "\n",
    "        # Stack all states efficiently\n",
    "        states = torch.cat(states)\n",
    "        next_states = torch.cat(next_states)\n",
    "        actions = torch.tensor(actions)\n",
    "        rewards = torch.tensor(rewards)\n",
    "        dones = torch.tensor(dones)\n",
    "\n",
    "        return states, actions, rewards, next_states, dones\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "    def clear(self):\n",
    "        self.buffer.clear()\n",
    "\n",
    "def preprocess_image(image):\n",
    "    pil_image = Image.fromarray(image)\n",
    "    width, height = pil_image.size\n",
    "    crop_height = int(height * 0.8)\n",
    "    cropped_image = pil_image.crop((0, 0, width, crop_height))\n",
    "    grayscale_image = cropped_image.convert(\"L\")\n",
    "    resized_image = grayscale_image.resize((84, 84))\n",
    "    preprocessed_image = np.array(resized_image) / 255.0\n",
    "    tensor_image = torch.FloatTensor(preprocessed_image).unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "    # Clean up PIL Images\n",
    "    del pil_image\n",
    "    del cropped_image\n",
    "    del grayscale_image\n",
    "    del resized_image\n",
    "\n",
    "    return tensor_image\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.policy_net = DQN(input_channels=4).to(self.device)\n",
    "        self.target_net = DQN(input_channels=4).to(self.device)\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        self.frame_stack = FrameStack(size=4)\n",
    "\n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=0.00025)\n",
    "        self.memory = ReplayBuffer(capacity=50000)\n",
    "\n",
    "        self.batch_size = 32\n",
    "        self.gamma = 0.99\n",
    "        self.epsilon_start = 0.05\n",
    "        self.epsilon_end = 0.01\n",
    "        self.epsilon_decay = 200000\n",
    "        self.current_epsilon = self.epsilon_start\n",
    "        self.target_update = 1000\n",
    "        self.frame_skip = 0\n",
    "\n",
    "        self.steps_done = 0\n",
    "\n",
    "    def select_action(self, state, training=True):\n",
    "        if self.steps_done % self.epsilon_decay == 0:\n",
    "            decay_amount = 0.001\n",
    "            self.current_epsilon = max(self.epsilon_end, self.current_epsilon - decay_amount)\n",
    "\n",
    "        if training and random.random() < self.current_epsilon:\n",
    "            return random.randint(0, 1)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            state = state.to(self.device)\n",
    "            q_values = self.policy_net(state)\n",
    "            action = q_values.max(1)[1].item()\n",
    "            # Clean up\n",
    "            del q_values\n",
    "            return action\n",
    "\n",
    "    def optimize_model(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "\n",
    "        # Sample and move to device\n",
    "        states, actions, rewards, next_states, dones = self.memory.sample(self.batch_size)\n",
    "        states = states.to(self.device)\n",
    "        actions = actions.to(self.device)\n",
    "        rewards = rewards.to(self.device)\n",
    "        next_states = next_states.to(self.device)\n",
    "        dones = dones.to(self.device)\n",
    "\n",
    "        # Compute loss\n",
    "        with torch.no_grad():\n",
    "            next_actions = self.policy_net(next_states).max(1)[1].unsqueeze(1)\n",
    "            next_q_values = self.target_net(next_states).gather(1, next_actions)\n",
    "            expected_q_values = rewards.unsqueeze(1) + (1 - dones.float().unsqueeze(1)) * self.gamma * next_q_values\n",
    "\n",
    "        current_q_values = self.policy_net(states).gather(1, actions.unsqueeze(1))\n",
    "        loss = nn.MSELoss()(current_q_values, expected_q_values)\n",
    "\n",
    "        # Optimize\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.policy_net.parameters(), 10)\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # Clean up\n",
    "        del states, actions, rewards, next_states, dones\n",
    "        del next_actions, next_q_values, expected_q_values, current_q_values\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    def update_hyperparameters(self):\n",
    "        \"\"\"Update hyperparameters based on the YAML configuration\"\"\"\n",
    "        try:\n",
    "            with open('hyperparameter_updates.yaml', 'r') as file:\n",
    "                config = yaml.safe_load(file)\n",
    "\n",
    "            if config['status'] == 'Update':\n",
    "                print(\"\\nUpdating hyperparameters...\")\n",
    "                updates = config['updates']\n",
    "\n",
    "                # Update learning rate\n",
    "                if 'learning_rate' in updates:\n",
    "                    lr_update = updates['learning_rate']\n",
    "                    current_lr = self.optimizer.param_groups[0]['lr']\n",
    "\n",
    "                    if lr_update['action'] == 'add':\n",
    "                        new_lr = current_lr + lr_update['value']\n",
    "                    elif lr_update['action'] == 'sub':\n",
    "                        new_lr = current_lr - lr_update['value']\n",
    "                    elif lr_update['action'] == 'mul':\n",
    "                        new_lr = current_lr * lr_update['value']\n",
    "\n",
    "                    for param_group in self.optimizer.param_groups:\n",
    "                        param_group['lr'] = new_lr\n",
    "                    print(f\"Learning rate updated: {current_lr:.4f} -> {new_lr:.7f}\")\n",
    "\n",
    "                # Update current epsilon\n",
    "                if 'epsilon' in updates:\n",
    "                    eps_update = updates['epsilon']\n",
    "                    old_epsilon = self.current_epsilon\n",
    "                    if eps_update['action'] == 'add':\n",
    "                        self.current_epsilon = min(1.0, self.current_epsilon + eps_update['value'])\n",
    "                    elif eps_update['action'] == 'sub':\n",
    "                        self.current_epsilon = max(self.epsilon_end, self.current_epsilon - eps_update['value'])\n",
    "                    print(f\"Current epsilon updated: {old_epsilon:.4f} -> {self.current_epsilon:.4f}\")\n",
    "\n",
    "                # Update epsilon decay\n",
    "                if 'epsilon_decay' in updates:\n",
    "                    decay_update = updates['epsilon_decay']\n",
    "                    old_decay = self.epsilon_decay\n",
    "                    if decay_update['action'] == 'mul':\n",
    "                        self.epsilon_decay = int(self.epsilon_decay * decay_update['value'])\n",
    "                    print(f\"Epsilon decay updated: {old_decay} -> {self.epsilon_decay}\")\n",
    "\n",
    "                # Set status to \"Ignore\" after applying updates\n",
    "                config['status'] = 'Ignore'\n",
    "                with open('hyperparameter_updates.yaml', 'w') as file:\n",
    "                    yaml.dump(config, file)\n",
    "\n",
    "                print(\"Hyperparameter updates complete\\n\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error updating hyperparameters: {str(e)}\")\n",
    "\n",
    "    def load_model(self, episode_number):\n",
    "        \"\"\"Load a previously saved model and handle architecture changes\"\"\"\n",
    "        model_path = os.path.join('models', f'dqn_episode_{episode_number}.pth')\n",
    "        if os.path.exists(model_path):\n",
    "            checkpoint = torch.load(model_path)\n",
    "\n",
    "            # Handle the case where we're loading a single-channel model into a four-channel model\n",
    "            old_state_dict = checkpoint['model_state_dict']\n",
    "            new_state_dict = self.policy_net.state_dict()\n",
    "\n",
    "            # Special handling for the first conv layer\n",
    "            if 'conv_layers.0.weight' in old_state_dict:\n",
    "                old_weights = old_state_dict['conv_layers.0.weight']\n",
    "                if old_weights.size(1) == 1 and new_state_dict['conv_layers.0.weight'].size(1) == 4:\n",
    "                    # Duplicate the single channel weights across all 4 channels\n",
    "                    new_weights = old_weights.repeat(1, 4, 1, 1)\n",
    "                    old_state_dict['conv_layers.0.weight'] = new_weights\n",
    "\n",
    "            # Load the modified state dict\n",
    "            self.policy_net.load_state_dict(old_state_dict)\n",
    "            self.target_net.load_state_dict(old_state_dict)\n",
    "            self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "            self.steps_done = episode_number * 1000  # Approximate steps based on episode\n",
    "            print(f\"Successfully loaded and adapted model from episode {episode_number}\")\n",
    "            return checkpoint['episode'], checkpoint['reward']\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"No model found at {model_path}\")\n",
    "\n",
    "    def clear_memory(self):\n",
    "        self.memory.clear()\n",
    "        self.frame_stack.clear()\n",
    "        gc.collect()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    def train(self, num_episodes=10000, start_episode=None, load_existing=True):\n",
    "        os.makedirs('models', exist_ok=True)\n",
    "        os.makedirs('images', exist_ok=True)\n",
    "    \n",
    "        rewards_history = deque(maxlen=100)\n",
    "        best_reward = float('-inf')\n",
    "    \n",
    "        if start_episode is not None and load_existing:\n",
    "            try:\n",
    "                episode_num, last_reward = self.load_model(start_episode)\n",
    "                best_reward = last_reward\n",
    "                start_episode = episode_num\n",
    "                print(\"Continuing training from episode\", start_episode)\n",
    "            except FileNotFoundError:\n",
    "                print(f\"No existing model found for episode {start_episode}, starting fresh\")\n",
    "                start_episode = 0\n",
    "        else:\n",
    "            start_episode = 0\n",
    "    \n",
    "        try:\n",
    "            for episode in range(start_episode, num_episodes):\n",
    "                state, _ = self.env.reset()\n",
    "                frame = preprocess_image(self.env.render())\n",
    "    \n",
    "                self.frame_stack = FrameStack(size=4)\n",
    "                for _ in range(4):\n",
    "                    self.frame_stack.push(frame)\n",
    "                state = self.frame_stack.get_state()\n",
    "    \n",
    "                episode_reward = 0\n",
    "                done = False\n",
    "    \n",
    "                while not done:\n",
    "                    action = self.select_action(state)\n",
    "    \n",
    "                    # Take at least one step regardless of frame_skip\n",
    "                    next_state, reward, done, truncated, _ = self.env.step(action)\n",
    "                    skip_reward = reward\n",
    "    \n",
    "                    # Additional frame skips if frame_skip > 0\n",
    "                    for _ in range(max(0, self.frame_skip - 1)):\n",
    "                        if done:\n",
    "                            break\n",
    "                        next_state, reward, done, truncated, _ = self.env.step(action)\n",
    "                        skip_reward += reward\n",
    "    \n",
    "                    next_frame = preprocess_image(self.env.render())\n",
    "                    self.frame_stack.push(next_frame)\n",
    "                    next_state = self.frame_stack.get_state()\n",
    "    \n",
    "                    self.memory.push(state, action, skip_reward, next_state, done)\n",
    "                    self.optimize_model()\n",
    "    \n",
    "                    state = next_state\n",
    "                    episode_reward += skip_reward\n",
    "                    self.steps_done += 1\n",
    "    \n",
    "                    if self.steps_done % self.target_update == 0:\n",
    "                        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "    \n",
    "                rewards_history.append(episode_reward)\n",
    "                avg_reward = np.mean(rewards_history)\n",
    "    \n",
    "                # Periodic cleanup\n",
    "                if episode % 100 == 0:\n",
    "                    gc.collect()\n",
    "                    if torch.cuda.is_available():\n",
    "                        torch.cuda.empty_cache()\n",
    "    \n",
    "                    # Automatically adjust epsilon based on average reward\n",
    "                    if len(rewards_history) == 100:  # Only adjust after we have 100 episodes\n",
    "                        if avg_reward > 20:\n",
    "                            self.current_epsilon = min(1.0, self.current_epsilon + 0.005)\n",
    "                            print(f\"Average reward above 20, increasing epsilon to {self.current_epsilon:.3f}\")\n",
    "\n",
    "                        elif avg_reward < 10:\n",
    "                            self.current_epsilon = max(self.epsilon_end, self.current_epsilon - 0.005)\n",
    "                            print(f\"Average reward below 10, decreasing epsilon to {self.current_epsilon:.3f}\")\n",
    "\n",
    "    \n",
    "                # Save model periodically\n",
    "                if episode % 100 == 0:\n",
    "                    self.clear_memory()  # Clear memory before saving\n",
    "                    model_path = os.path.join('models', f'dqn_episode_{episode}.pth')\n",
    "                    torch.save({\n",
    "                        'episode': episode,\n",
    "                        'model_state_dict': self.policy_net.state_dict(),\n",
    "                        'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "                        'reward': episode_reward,\n",
    "                    }, model_path)\n",
    "                    print(f\"Model saved at episode {episode}\")\n",
    "    \n",
    "                if episode_reward > best_reward:\n",
    "                    best_reward = episode_reward\n",
    "                    best_model_path = os.path.join('models', 'dqn_best.pth')\n",
    "                    torch.save({\n",
    "                        'episode': episode,\n",
    "                        'model_state_dict': self.policy_net.state_dict(),\n",
    "                        'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "                        'reward': episode_reward,\n",
    "                    }, best_model_path)\n",
    "                    print(f\"New best model saved. Episode {episode} with reward {episode_reward:.2f}\")\n",
    "    \n",
    "                if (episode + 1) % 100 == 0:\n",
    "                    print(f\"Episode {episode + 1}/{num_episodes} - \"\n",
    "                          f\"Reward: {episode_reward:.2f} - \"\n",
    "                          f\"Average Reward (100 ep): {avg_reward:.2f} - \"\n",
    "                          f\"Best Reward: {best_reward:.2f} - \"\n",
    "                          f\"Epsilon: {self.current_epsilon:.7f} - \"\n",
    "                          f\"Learning Rate: {self.optimizer.param_groups[0]['lr']:.7f}\")\n",
    "    \n",
    "                self.update_hyperparameters()\n",
    "    \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\nTraining interrupted. Cleaning up...\")\n",
    "            self.clear_memory()\n",
    "    \n",
    "        finally:\n",
    "            self.clear_memory()\n",
    "\n",
    "def main():\n",
    "    env = gym.make(\"FlappyBird-v0\", render_mode=\"rgb_array\", use_lidar=False)\n",
    "    agent = DQNAgent(env)\n",
    "\n",
    "    try:\n",
    "        agent.train(start_episode=7200)\n",
    "    finally:\n",
    "        env.close()\n",
    "        gc.collect()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved at episode 0\n",
      "New best model saved. Episode 0 with reward 3.90\n",
      "New best model saved. Episode 93 with reward 4.10\n",
      "Episode 100/10000 - Reward: 3.90 - Average Reward (100 ep): -5.10 - Best Reward: 4.10 - Epsilon: 0.2990000 - Learning Rate: 0.0002500\n",
      "Average reward below 10, decreasing epsilon to 0.294\n",
      "Model saved at episode 100\n",
      "Episode 200/10000 - Reward: -2.70 - Average Reward (100 ep): -4.46 - Best Reward: 4.10 - Epsilon: 0.2940000 - Learning Rate: 0.0002500\n",
      "Average reward below 10, decreasing epsilon to 0.289\n",
      "Model saved at episode 200\n",
      "Episode 300/10000 - Reward: 3.90 - Average Reward (100 ep): -2.02 - Best Reward: 4.10 - Epsilon: 0.2890000 - Learning Rate: 0.0002500\n",
      "Average reward below 10, decreasing epsilon to 0.284\n",
      "Model saved at episode 300\n",
      "Episode 400/10000 - Reward: 0.30 - Average Reward (100 ep): -0.46 - Best Reward: 4.10 - Epsilon: 0.2840000 - Learning Rate: 0.0002500\n",
      "Average reward below 10, decreasing epsilon to 0.279\n",
      "Model saved at episode 400\n",
      "Episode 500/10000 - Reward: -0.30 - Average Reward (100 ep): -0.72 - Best Reward: 4.10 - Epsilon: 0.2790000 - Learning Rate: 0.0002500\n",
      "Average reward below 10, decreasing epsilon to 0.274\n",
      "Model saved at episode 500\n",
      "Episode 600/10000 - Reward: -7.50 - Average Reward (100 ep): -1.08 - Best Reward: 4.10 - Epsilon: 0.2740000 - Learning Rate: 0.0002500\n",
      "Average reward below 10, decreasing epsilon to 0.269\n",
      "Model saved at episode 600\n",
      "Episode 700/10000 - Reward: -8.70 - Average Reward (100 ep): -2.81 - Best Reward: 4.10 - Epsilon: 0.2690000 - Learning Rate: 0.0002500\n",
      "Average reward below 10, decreasing epsilon to 0.264\n",
      "Model saved at episode 700\n",
      "New best model saved. Episode 755 with reward 4.40\n",
      "Episode 800/10000 - Reward: -4.50 - Average Reward (100 ep): -2.62 - Best Reward: 4.40 - Epsilon: 0.2640000 - Learning Rate: 0.0002500\n",
      "Average reward below 10, decreasing epsilon to 0.259\n",
      "Model saved at episode 800\n",
      "Episode 900/10000 - Reward: 3.30 - Average Reward (100 ep): -3.55 - Best Reward: 4.40 - Epsilon: 0.2590000 - Learning Rate: 0.0002500\n",
      "Average reward below 10, decreasing epsilon to 0.254\n",
      "Model saved at episode 900\n",
      "Episode 1000/10000 - Reward: 3.90 - Average Reward (100 ep): -3.22 - Best Reward: 4.40 - Epsilon: 0.2540000 - Learning Rate: 0.0002500\n",
      "Average reward below 10, decreasing epsilon to 0.249\n",
      "Model saved at episode 1000\n",
      "Episode 1100/10000 - Reward: -5.70 - Average Reward (100 ep): -3.28 - Best Reward: 4.40 - Epsilon: 0.2490000 - Learning Rate: 0.0002500\n",
      "Average reward below 10, decreasing epsilon to 0.244\n",
      "Model saved at episode 1100\n",
      "Episode 1200/10000 - Reward: 3.90 - Average Reward (100 ep): -1.03 - Best Reward: 4.40 - Epsilon: 0.2440000 - Learning Rate: 0.0002500\n",
      "Average reward below 10, decreasing epsilon to 0.239\n",
      "Model saved at episode 1200\n",
      "Episode 1300/10000 - Reward: -5.10 - Average Reward (100 ep): -0.70 - Best Reward: 4.40 - Epsilon: 0.2390000 - Learning Rate: 0.0002500\n",
      "Average reward below 10, decreasing epsilon to 0.234\n",
      "Model saved at episode 1300\n",
      "New best model saved. Episode 1382 with reward 4.50\n",
      "Episode 1400/10000 - Reward: -1.50 - Average Reward (100 ep): -1.88 - Best Reward: 4.50 - Epsilon: 0.2340000 - Learning Rate: 0.0002500\n",
      "Average reward below 10, decreasing epsilon to 0.229\n",
      "Model saved at episode 1400\n",
      "Episode 1500/10000 - Reward: -5.70 - Average Reward (100 ep): -2.80 - Best Reward: 4.50 - Epsilon: 0.2290000 - Learning Rate: 0.0002500\n",
      "Average reward below 10, decreasing epsilon to 0.224\n",
      "Model saved at episode 1500\n",
      "Episode 1600/10000 - Reward: -5.10 - Average Reward (100 ep): -2.26 - Best Reward: 4.50 - Epsilon: 0.2240000 - Learning Rate: 0.0002500\n",
      "Average reward below 10, decreasing epsilon to 0.219\n",
      "Model saved at episode 1600\n",
      "New best model saved. Episode 1656 with reward 4.90\n",
      "Episode 1700/10000 - Reward: -6.90 - Average Reward (100 ep): -1.11 - Best Reward: 4.90 - Epsilon: 0.2190000 - Learning Rate: 0.0002500\n",
      "Average reward below 10, decreasing epsilon to 0.214\n",
      "Model saved at episode 1700\n",
      "New best model saved. Episode 1749 with reward 5.00\n",
      "New best model saved. Episode 1768 with reward 8.40\n",
      "Episode 1800/10000 - Reward: 3.90 - Average Reward (100 ep): -1.45 - Best Reward: 8.40 - Epsilon: 0.2140000 - Learning Rate: 0.0002500\n",
      "Average reward below 10, decreasing epsilon to 0.209\n",
      "Model saved at episode 1800\n",
      "Episode 1900/10000 - Reward: -5.70 - Average Reward (100 ep): -0.96 - Best Reward: 8.40 - Epsilon: 0.2090000 - Learning Rate: 0.0002500\n",
      "Average reward below 10, decreasing epsilon to 0.204\n",
      "Model saved at episode 1900\n",
      "Episode 2000/10000 - Reward: 3.90 - Average Reward (100 ep): -0.34 - Best Reward: 8.40 - Epsilon: 0.2040000 - Learning Rate: 0.0002500\n",
      "Average reward below 10, decreasing epsilon to 0.199\n",
      "Model saved at episode 2000\n",
      "Episode 2100/10000 - Reward: 3.90 - Average Reward (100 ep): 0.39 - Best Reward: 8.40 - Epsilon: 0.1990000 - Learning Rate: 0.0002500\n",
      "Average reward below 10, decreasing epsilon to 0.194\n",
      "Model saved at episode 2100\n",
      "New best model saved. Episode 2166 with reward 10.80\n",
      "Episode 2200/10000 - Reward: -2.70 - Average Reward (100 ep): 1.35 - Best Reward: 10.80 - Epsilon: 0.1940000 - Learning Rate: 0.0002500\n",
      "Average reward below 10, decreasing epsilon to 0.189\n",
      "Model saved at episode 2200\n",
      "Episode 2300/10000 - Reward: -6.90 - Average Reward (100 ep): 1.37 - Best Reward: 10.80 - Epsilon: 0.1890000 - Learning Rate: 0.0002500\n",
      "Average reward below 10, decreasing epsilon to 0.184\n",
      "Model saved at episode 2300\n",
      "Episode 2400/10000 - Reward: -6.90 - Average Reward (100 ep): 0.25 - Best Reward: 10.80 - Epsilon: 0.1840000 - Learning Rate: 0.0002500\n",
      "Average reward below 10, decreasing epsilon to 0.179\n",
      "Model saved at episode 2400\n",
      "Episode 2500/10000 - Reward: -0.30 - Average Reward (100 ep): 0.30 - Best Reward: 10.80 - Epsilon: 0.1790000 - Learning Rate: 0.0002500\n",
      "Average reward below 10, decreasing epsilon to 0.174\n",
      "Model saved at episode 2500\n",
      "Episode 2600/10000 - Reward: -5.70 - Average Reward (100 ep): 0.39 - Best Reward: 10.80 - Epsilon: 0.1740000 - Learning Rate: 0.0002500\n",
      "Average reward below 10, decreasing epsilon to 0.169\n",
      "Model saved at episode 2600\n",
      "Episode 2700/10000 - Reward: 3.90 - Average Reward (100 ep): 0.35 - Best Reward: 10.80 - Epsilon: 0.1690000 - Learning Rate: 0.0002500\n",
      "Average reward below 10, decreasing epsilon to 0.164\n",
      "Model saved at episode 2700\n",
      "Episode 2800/10000 - Reward: -5.70 - Average Reward (100 ep): 0.33 - Best Reward: 10.80 - Epsilon: 0.1640000 - Learning Rate: 0.0002500\n",
      "Average reward below 10, decreasing epsilon to 0.159\n",
      "Model saved at episode 2800\n",
      "Episode 2900/10000 - Reward: -3.90 - Average Reward (100 ep): 0.77 - Best Reward: 10.80 - Epsilon: 0.1590000 - Learning Rate: 0.0002500\n",
      "Average reward below 10, decreasing epsilon to 0.154\n",
      "Model saved at episode 2900\n",
      "Episode 3000/10000 - Reward: -6.90 - Average Reward (100 ep): -0.03 - Best Reward: 10.80 - Epsilon: 0.1540000 - Learning Rate: 0.0002500\n",
      "Average reward below 10, decreasing epsilon to 0.149\n",
      "Model saved at episode 3000\n",
      "New best model saved. Episode 3008 with reward 12.90\n",
      "Episode 3100/10000 - Reward: 9.20 - Average Reward (100 ep): -0.10 - Best Reward: 12.90 - Epsilon: 0.1490000 - Learning Rate: 0.0002500\n",
      "Average reward below 10, decreasing epsilon to 0.144\n",
      "Model saved at episode 3100\n",
      "Episode 3200/10000 - Reward: -7.50 - Average Reward (100 ep): -0.33 - Best Reward: 12.90 - Epsilon: 0.1440000 - Learning Rate: 0.0002500\n",
      "Average reward below 10, decreasing epsilon to 0.139\n",
      "Model saved at episode 3200\n",
      "New best model saved. Episode 3299 with reward 13.80\n",
      "Episode 3300/10000 - Reward: 13.80 - Average Reward (100 ep): 0.69 - Best Reward: 13.80 - Epsilon: 0.1390000 - Learning Rate: 0.0002500\n",
      "Average reward below 10, decreasing epsilon to 0.134\n",
      "Model saved at episode 3300\n",
      "Episode 3400/10000 - Reward: -5.70 - Average Reward (100 ep): 0.43 - Best Reward: 13.80 - Epsilon: 0.1340000 - Learning Rate: 0.0002500\n",
      "Average reward below 10, decreasing epsilon to 0.129\n",
      "Model saved at episode 3400\n",
      "Episode 3500/10000 - Reward: -5.70 - Average Reward (100 ep): 1.81 - Best Reward: 13.80 - Epsilon: 0.1290000 - Learning Rate: 0.0002500\n",
      "Average reward below 10, decreasing epsilon to 0.124\n",
      "Model saved at episode 3500\n",
      "Episode 3600/10000 - Reward: 3.90 - Average Reward (100 ep): 2.72 - Best Reward: 13.80 - Epsilon: 0.1240000 - Learning Rate: 0.0002500\n",
      "Average reward below 10, decreasing epsilon to 0.119\n",
      "Model saved at episode 3600\n",
      "Episode 3700/10000 - Reward: 8.40 - Average Reward (100 ep): 3.63 - Best Reward: 13.80 - Epsilon: 0.1190000 - Learning Rate: 0.0002500\n",
      "Average reward below 10, decreasing epsilon to 0.114\n",
      "Model saved at episode 3700\n",
      "Episode 3800/10000 - Reward: 3.90 - Average Reward (100 ep): 2.41 - Best Reward: 13.80 - Epsilon: 0.1140000 - Learning Rate: 0.0002500\n",
      "Average reward below 10, decreasing epsilon to 0.109\n",
      "Model saved at episode 3800\n",
      "Episode 3900/10000 - Reward: 3.90 - Average Reward (100 ep): 3.09 - Best Reward: 13.80 - Epsilon: 0.1090000 - Learning Rate: 0.0002500\n",
      "Average reward below 10, decreasing epsilon to 0.104\n",
      "Model saved at episode 3900\n",
      "New best model saved. Episode 3937 with reward 20.70\n",
      "Episode 4000/10000 - Reward: 3.90 - Average Reward (100 ep): 3.96 - Best Reward: 20.70 - Epsilon: 0.1030000 - Learning Rate: 0.0002500\n",
      "Average reward below 10, decreasing epsilon to 0.098\n",
      "Model saved at episode 4000\n",
      "Episode 4100/10000 - Reward: 8.40 - Average Reward (100 ep): 3.68 - Best Reward: 20.70 - Epsilon: 0.0980000 - Learning Rate: 0.0002500\n",
      "Average reward below 10, decreasing epsilon to 0.093\n",
      "Model saved at episode 4100\n",
      "New best model saved. Episode 4179 with reward 24.70\n",
      "Episode 4200/10000 - Reward: -4.50 - Average Reward (100 ep): 4.52 - Best Reward: 24.70 - Epsilon: 0.0930000 - Learning Rate: 0.0002500\n",
      "Average reward below 10, decreasing epsilon to 0.088\n",
      "Model saved at episode 4200\n",
      "Episode 4300/10000 - Reward: 2.70 - Average Reward (100 ep): 3.14 - Best Reward: 24.70 - Epsilon: 0.0880000 - Learning Rate: 0.0002500\n",
      "Average reward below 10, decreasing epsilon to 0.083\n",
      "Model saved at episode 4300\n",
      "New best model saved. Episode 4360 with reward 24.90\n",
      "New best model saved. Episode 4371 with reward 25.00\n",
      "Episode 4400/10000 - Reward: 8.40 - Average Reward (100 ep): 4.43 - Best Reward: 25.00 - Epsilon: 0.0830000 - Learning Rate: 0.0002500\n",
      "Average reward below 10, decreasing epsilon to 0.078\n",
      "Model saved at episode 4400\n",
      "New best model saved. Episode 4488 with reward 26.90\n",
      "Episode 4500/10000 - Reward: 8.80 - Average Reward (100 ep): 4.67 - Best Reward: 26.90 - Epsilon: 0.0780000 - Learning Rate: 0.0002500\n",
      "Average reward below 10, decreasing epsilon to 0.073\n",
      "Model saved at episode 4500\n",
      "Episode 4600/10000 - Reward: 4.20 - Average Reward (100 ep): 4.98 - Best Reward: 26.90 - Epsilon: 0.0730000 - Learning Rate: 0.0002500\n",
      "Average reward below 10, decreasing epsilon to 0.068\n",
      "Model saved at episode 4600\n",
      "Episode 4700/10000 - Reward: 4.40 - Average Reward (100 ep): 4.85 - Best Reward: 26.90 - Epsilon: 0.0680000 - Learning Rate: 0.0002500\n",
      "Average reward below 10, decreasing epsilon to 0.063\n",
      "Model saved at episode 4700\n",
      "Episode 4800/10000 - Reward: 4.70 - Average Reward (100 ep): 5.73 - Best Reward: 26.90 - Epsilon: 0.0630000 - Learning Rate: 0.0002500\n",
      "Average reward below 10, decreasing epsilon to 0.058\n",
      "Model saved at episode 4800\n",
      "New best model saved. Episode 4841 with reward 29.30\n",
      "New best model saved. Episode 4884 with reward 36.40\n",
      "Episode 4900/10000 - Reward: 15.80 - Average Reward (100 ep): 6.92 - Best Reward: 36.40 - Epsilon: 0.0580000 - Learning Rate: 0.0002500\n",
      "Average reward below 10, decreasing epsilon to 0.053\n",
      "Model saved at episode 4900\n",
      "Episode 5000/10000 - Reward: 4.80 - Average Reward (100 ep): 7.51 - Best Reward: 36.40 - Epsilon: 0.0530000 - Learning Rate: 0.0002500\n",
      "Average reward below 10, decreasing epsilon to 0.048\n",
      "Model saved at episode 5000\n",
      "New best model saved. Episode 5057 with reward 46.40\n",
      "Episode 5100/10000 - Reward: 3.90 - Average Reward (100 ep): 7.28 - Best Reward: 46.40 - Epsilon: 0.0480000 - Learning Rate: 0.0002500\n",
      "Average reward below 10, decreasing epsilon to 0.043\n",
      "Model saved at episode 5100\n",
      "New best model saved. Episode 5143 with reward 61.10\n",
      "Episode 5200/10000 - Reward: 3.90 - Average Reward (100 ep): 9.54 - Best Reward: 61.10 - Epsilon: 0.0430000 - Learning Rate: 0.0002500\n",
      "Average reward below 10, decreasing epsilon to 0.038\n",
      "Model saved at episode 5200\n",
      "Episode 5300/10000 - Reward: 17.90 - Average Reward (100 ep): 11.95 - Best Reward: 61.10 - Epsilon: 0.0380000 - Learning Rate: 0.0002500\n",
      "Model saved at episode 5300\n",
      "New best model saved. Episode 5354 with reward 64.40\n",
      "Episode 5400/10000 - Reward: 3.90 - Average Reward (100 ep): 13.03 - Best Reward: 64.40 - Epsilon: 0.0380000 - Learning Rate: 0.0002500\n",
      "Model saved at episode 5400\n",
      "Episode 5500/10000 - Reward: 11.30 - Average Reward (100 ep): 12.23 - Best Reward: 64.40 - Epsilon: 0.0380000 - Learning Rate: 0.0002500\n",
      "Model saved at episode 5500\n",
      "Episode 5600/10000 - Reward: 15.10 - Average Reward (100 ep): 10.72 - Best Reward: 64.40 - Epsilon: 0.0380000 - Learning Rate: 0.0002500\n",
      "Model saved at episode 5600\n",
      "Episode 5700/10000 - Reward: 26.90 - Average Reward (100 ep): 11.25 - Best Reward: 64.40 - Epsilon: 0.0380000 - Learning Rate: 0.0002500\n",
      "Model saved at episode 5700\n",
      "Episode 5800/10000 - Reward: 6.80 - Average Reward (100 ep): 12.12 - Best Reward: 64.40 - Epsilon: 0.0380000 - Learning Rate: 0.0002500\n",
      "Model saved at episode 5800\n",
      "Episode 5900/10000 - Reward: 3.90 - Average Reward (100 ep): 13.98 - Best Reward: 64.40 - Epsilon: 0.0380000 - Learning Rate: 0.0002500\n",
      "Model saved at episode 5900\n",
      "Episode 6000/10000 - Reward: 4.80 - Average Reward (100 ep): 11.04 - Best Reward: 64.40 - Epsilon: 0.0380000 - Learning Rate: 0.0002500\n",
      "Model saved at episode 6000\n",
      "\n",
      "Updating hyperparameters...\n",
      "Learning rate updated: 0.0003 -> 0.0002500\n",
      "Current epsilon updated: 0.0380 -> 0.0100\n",
      "Epsilon decay updated: 200000 -> 200000\n",
      "Hyperparameter updates complete\n",
      "\n",
      "New best model saved. Episode 6093 with reward 102.00\n",
      "Episode 6100/10000 - Reward: 34.60 - Average Reward (100 ep): 12.72 - Best Reward: 102.00 - Epsilon: 0.0100000 - Learning Rate: 0.0002500\n",
      "Model saved at episode 6100\n",
      "New best model saved. Episode 6113 with reward 115.90\n",
      "\n",
      "Updating hyperparameters...\n",
      "Learning rate updated: 0.0003 -> 0.0002500\n",
      "Current epsilon updated: 0.0100 -> 0.0100\n",
      "Epsilon decay updated: 200000 -> 200000\n",
      "Hyperparameter updates complete\n",
      "\n",
      "\n",
      "Updating hyperparameters...\n",
      "Learning rate updated: 0.0003 -> 0.0002500\n",
      "Current epsilon updated: 0.0100 -> 0.0100\n",
      "Epsilon decay updated: 200000 -> 200000\n",
      "Hyperparameter updates complete\n",
      "\n",
      "New best model saved. Episode 6145 with reward 124.90\n",
      "New best model saved. Episode 6172 with reward 180.90\n",
      "Episode 6200/10000 - Reward: 3.90 - Average Reward (100 ep): 33.75 - Best Reward: 180.90 - Epsilon: 0.0100000 - Learning Rate: 0.0002500\n",
      "Average reward above 20, increasing epsilon to 0.015\n",
      "Model saved at episode 6200\n",
      "Episode 6300/10000 - Reward: 40.90 - Average Reward (100 ep): 27.24 - Best Reward: 180.90 - Epsilon: 0.0150000 - Learning Rate: 0.0002500\n",
      "Average reward above 20, increasing epsilon to 0.020\n",
      "Model saved at episode 6300\n",
      "Episode 6400/10000 - Reward: 6.10 - Average Reward (100 ep): 15.36 - Best Reward: 180.90 - Epsilon: 0.0200000 - Learning Rate: 0.0002500\n",
      "Model saved at episode 6400\n",
      "Episode 6500/10000 - Reward: 27.60 - Average Reward (100 ep): 23.97 - Best Reward: 180.90 - Epsilon: 0.0200000 - Learning Rate: 0.0002500\n",
      "Average reward above 20, increasing epsilon to 0.025\n",
      "Model saved at episode 6500\n",
      "Episode 6600/10000 - Reward: 41.80 - Average Reward (100 ep): 17.87 - Best Reward: 180.90 - Epsilon: 0.0250000 - Learning Rate: 0.0002500\n",
      "Model saved at episode 6600\n",
      "Episode 6700/10000 - Reward: 29.40 - Average Reward (100 ep): 18.32 - Best Reward: 180.90 - Epsilon: 0.0250000 - Learning Rate: 0.0002500\n",
      "Model saved at episode 6700\n",
      "Episode 6800/10000 - Reward: 34.30 - Average Reward (100 ep): 16.22 - Best Reward: 180.90 - Epsilon: 0.0250000 - Learning Rate: 0.0002500\n",
      "Model saved at episode 6800\n",
      "Episode 6900/10000 - Reward: 43.30 - Average Reward (100 ep): 16.84 - Best Reward: 180.90 - Epsilon: 0.0250000 - Learning Rate: 0.0002500\n",
      "Model saved at episode 6900\n",
      "\n",
      "Updating hyperparameters...\n",
      "Learning rate updated: 0.0003 -> 0.0001250\n",
      "Current epsilon updated: 0.0250 -> 0.0250\n",
      "Epsilon decay updated: 200000 -> 200000\n",
      "Hyperparameter updates complete\n",
      "\n",
      "Episode 7000/10000 - Reward: 4.40 - Average Reward (100 ep): 18.29 - Best Reward: 180.90 - Epsilon: 0.0250000 - Learning Rate: 0.0001250\n",
      "Model saved at episode 7000\n",
      "Episode 7100/10000 - Reward: 3.90 - Average Reward (100 ep): 15.19 - Best Reward: 180.90 - Epsilon: 0.0250000 - Learning Rate: 0.0001250\n",
      "Model saved at episode 7100\n",
      "Episode 7200/10000 - Reward: 4.60 - Average Reward (100 ep): 16.37 - Best Reward: 180.90 - Epsilon: 0.0250000 - Learning Rate: 0.0001250\n",
      "Model saved at episode 7200\n",
      "Episode 7300/10000 - Reward: 36.40 - Average Reward (100 ep): 19.38 - Best Reward: 180.90 - Epsilon: 0.0240000 - Learning Rate: 0.0001250\n",
      "Model saved at episode 7300\n",
      "Episode 7400/10000 - Reward: 22.70 - Average Reward (100 ep): 17.25 - Best Reward: 180.90 - Epsilon: 0.0240000 - Learning Rate: 0.0001250\n",
      "Model saved at episode 7400\n",
      "\n",
      "Updating hyperparameters...\n",
      "Learning rate updated: 0.0001 -> 0.0001250\n",
      "Current epsilon updated: 0.0240 -> 0.0240\n",
      "Epsilon decay updated: 200000 -> 200000\n",
      "Hyperparameter updates complete\n",
      "\n",
      "\n",
      "Updating hyperparameters...\n",
      "Learning rate updated: 0.0001 -> 0.0001250\n",
      "Current epsilon updated: 0.0240 -> 0.0240\n",
      "Epsilon decay updated: 200000 -> 200000\n",
      "Hyperparameter updates complete\n",
      "\n",
      "\n",
      "Updating hyperparameters...\n",
      "Learning rate updated: 0.0001 -> 0.0001250\n",
      "Current epsilon updated: 0.0240 -> 0.0240\n",
      "Epsilon decay updated: 200000 -> 200000\n",
      "Hyperparameter updates complete\n",
      "\n",
      "\n",
      "Updating hyperparameters...\n",
      "Learning rate updated: 0.0001 -> 0.0001250\n",
      "Current epsilon updated: 0.0240 -> 0.0290\n",
      "Epsilon decay updated: 200000 -> 200000\n",
      "Hyperparameter updates complete\n",
      "\n",
      "Episode 7500/10000 - Reward: 11.20 - Average Reward (100 ep): 17.53 - Best Reward: 180.90 - Epsilon: 0.0290000 - Learning Rate: 0.0001250\n",
      "Model saved at episode 7500\n",
      "Episode 7600/10000 - Reward: 8.40 - Average Reward (100 ep): 15.24 - Best Reward: 180.90 - Epsilon: 0.0290000 - Learning Rate: 0.0001250\n",
      "Model saved at episode 7600\n",
      "Episode 7700/10000 - Reward: 17.90 - Average Reward (100 ep): 16.83 - Best Reward: 180.90 - Epsilon: 0.0290000 - Learning Rate: 0.0001250\n",
      "Model saved at episode 7700\n",
      "\n",
      "Updating hyperparameters...\n",
      "Learning rate updated: 0.0001 -> 0.0001250\n",
      "Current epsilon updated: 0.0290 -> 0.0390\n",
      "Epsilon decay updated: 200000 -> 200000\n",
      "Hyperparameter updates complete\n",
      "\n",
      "Episode 7800/10000 - Reward: 23.10 - Average Reward (100 ep): 15.23 - Best Reward: 180.90 - Epsilon: 0.0390000 - Learning Rate: 0.0001250\n",
      "Model saved at episode 7800\n",
      "Episode 7900/10000 - Reward: 17.90 - Average Reward (100 ep): 9.93 - Best Reward: 180.90 - Epsilon: 0.0390000 - Learning Rate: 0.0001250\n",
      "Average reward below 10, decreasing epsilon to 0.034\n",
      "Model saved at episode 7900\n",
      "Episode 8000/10000 - Reward: 15.60 - Average Reward (100 ep): 12.09 - Best Reward: 180.90 - Epsilon: 0.0340000 - Learning Rate: 0.0001250\n",
      "Model saved at episode 8000\n",
      "Episode 8100/10000 - Reward: 9.50 - Average Reward (100 ep): 12.31 - Best Reward: 180.90 - Epsilon: 0.0340000 - Learning Rate: 0.0001250\n",
      "Model saved at episode 8100\n",
      "Episode 8200/10000 - Reward: 3.90 - Average Reward (100 ep): 11.96 - Best Reward: 180.90 - Epsilon: 0.0340000 - Learning Rate: 0.0001250\n",
      "Model saved at episode 8200\n",
      "Episode 8300/10000 - Reward: 10.90 - Average Reward (100 ep): 11.83 - Best Reward: 180.90 - Epsilon: 0.0340000 - Learning Rate: 0.0001250\n",
      "Model saved at episode 8300\n",
      "Episode 8400/10000 - Reward: 24.80 - Average Reward (100 ep): 12.08 - Best Reward: 180.90 - Epsilon: 0.0340000 - Learning Rate: 0.0001250\n",
      "Model saved at episode 8400\n",
      "Episode 8500/10000 - Reward: 8.60 - Average Reward (100 ep): 12.63 - Best Reward: 180.90 - Epsilon: 0.0340000 - Learning Rate: 0.0001250\n",
      "Model saved at episode 8500\n",
      "Episode 8600/10000 - Reward: 3.90 - Average Reward (100 ep): 12.61 - Best Reward: 180.90 - Epsilon: 0.0340000 - Learning Rate: 0.0001250\n",
      "Model saved at episode 8600\n",
      "\n",
      "Updating hyperparameters...\n",
      "Learning rate updated: 0.0001 -> 0.0001250\n",
      "Current epsilon updated: 0.0340 -> 0.0320\n",
      "Epsilon decay updated: 200000 -> 200000\n",
      "Hyperparameter updates complete\n",
      "\n",
      "Episode 8700/10000 - Reward: 20.60 - Average Reward (100 ep): 10.83 - Best Reward: 180.90 - Epsilon: 0.0320000 - Learning Rate: 0.0001250\n",
      "Model saved at episode 8700\n",
      "\n",
      "Updating hyperparameters...\n",
      "Learning rate updated: 0.0001 -> 0.0002500\n",
      "Current epsilon updated: 0.0320 -> 0.0300\n",
      "Epsilon decay updated: 200000 -> 200000\n",
      "Hyperparameter updates complete\n",
      "\n",
      "\n",
      "Updating hyperparameters...\n",
      "Learning rate updated: 0.0003 -> 0.0002500\n",
      "Current epsilon updated: 0.0300 -> 0.0280\n",
      "Epsilon decay updated: 200000 -> 200000\n",
      "Hyperparameter updates complete\n",
      "\n",
      "Episode 8800/10000 - Reward: 8.40 - Average Reward (100 ep): 14.47 - Best Reward: 180.90 - Epsilon: 0.0280000 - Learning Rate: 0.0002500\n",
      "Model saved at episode 8800\n",
      "Episode 8900/10000 - Reward: 3.90 - Average Reward (100 ep): 15.27 - Best Reward: 180.90 - Epsilon: 0.0270000 - Learning Rate: 0.0002500\n",
      "Model saved at episode 8900\n",
      "Episode 9000/10000 - Reward: 29.60 - Average Reward (100 ep): 13.55 - Best Reward: 180.90 - Epsilon: 0.0270000 - Learning Rate: 0.0002500\n",
      "Model saved at episode 9000\n",
      "Episode 9100/10000 - Reward: 17.90 - Average Reward (100 ep): 16.80 - Best Reward: 180.90 - Epsilon: 0.0270000 - Learning Rate: 0.0002500\n",
      "Model saved at episode 9100\n"
     ]
    }
   ],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
