{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-23T21:08:11.413571Z",
     "start_time": "2024-12-23T17:39:53.876244Z"
    }
   },
   "source": [
    "import gymnasium as gym\n",
    "import flappy_bird_gymnasium\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import random\n",
    "from PIL import Image\n",
    "import os\n",
    "from datetime import datetime\n",
    "import yaml\n",
    "import gc\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_channels=4, n_actions=2):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, 32, kernel_size=8, stride=4),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.LeakyReLU(0.01)\n",
    "        )\n",
    "        \n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(3136, 512),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.Linear(512, n_actions)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc_layers(x)\n",
    "\n",
    "class FrameStack:\n",
    "    def __init__(self, size=4):\n",
    "        self.size = size\n",
    "        self.frames = deque(maxlen=size)\n",
    "    \n",
    "    def push(self, frame):\n",
    "        # Ensure frame is detached from computation graph and on CPU\n",
    "        if torch.is_tensor(frame):\n",
    "            frame = frame.cpu().detach()\n",
    "        self.frames.append(frame)\n",
    "    \n",
    "    def get_state(self):\n",
    "        while len(self.frames) < self.size:\n",
    "            if len(self.frames) > 0:\n",
    "                self.frames.append(self.frames[-1])\n",
    "            else:\n",
    "                zero_frame = torch.zeros(1, 1, 84, 84)\n",
    "                self.frames.append(zero_frame)\n",
    "        \n",
    "        # Stack frames and detach to prevent memory leaks\n",
    "        stacked = torch.cat(list(self.frames), dim=1)\n",
    "        return stacked.detach()\n",
    "\n",
    "    def clear(self):\n",
    "        self.frames.clear()\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity=10000):  # Reduced capacity\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "        \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        # Move tensors to CPU and detach from computation graph\n",
    "        if torch.is_tensor(state):\n",
    "            state = state.cpu().detach()\n",
    "        if torch.is_tensor(next_state):\n",
    "            next_state = next_state.cpu().detach()\n",
    "        \n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        samples = random.sample(self.buffer, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*samples)\n",
    "        \n",
    "        # Stack all states efficiently\n",
    "        states = torch.cat(states)\n",
    "        next_states = torch.cat(next_states)\n",
    "        actions = torch.tensor(actions)\n",
    "        rewards = torch.tensor(rewards)\n",
    "        dones = torch.tensor(dones)\n",
    "        \n",
    "        return states, actions, rewards, next_states, dones\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "    def clear(self):\n",
    "        self.buffer.clear()\n",
    "\n",
    "def preprocess_image(image):\n",
    "    pil_image = Image.fromarray(image)\n",
    "    width, height = pil_image.size\n",
    "    crop_height = int(height * 0.8)\n",
    "    cropped_image = pil_image.crop((0, 0, width, crop_height))\n",
    "    grayscale_image = cropped_image.convert(\"L\")\n",
    "    resized_image = grayscale_image.resize((84, 84))\n",
    "    preprocessed_image = np.array(resized_image) / 255.0\n",
    "    tensor_image = torch.FloatTensor(preprocessed_image).unsqueeze(0).unsqueeze(0)\n",
    "    \n",
    "    # Clean up PIL Images\n",
    "    del pil_image\n",
    "    del cropped_image\n",
    "    del grayscale_image\n",
    "    del resized_image\n",
    "    \n",
    "    return tensor_image\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.policy_net = DQN(input_channels=4).to(self.device)\n",
    "        self.target_net = DQN(input_channels=4).to(self.device)\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        self.frame_stack = FrameStack(size=4)\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=0.00025)\n",
    "        self.memory = ReplayBuffer(capacity=50000)\n",
    "        \n",
    "        self.batch_size = 32\n",
    "        self.gamma = 0.99\n",
    "        self.epsilon_start = 0.4\n",
    "        self.epsilon_end = 0.05\n",
    "        self.epsilon_decay = 200\n",
    "        self.current_epsilon = self.epsilon_start\n",
    "        self.target_update = 1000\n",
    "        self.frame_skip = 1\n",
    "        \n",
    "        self.steps_done = 0\n",
    "        \n",
    "    def select_action(self, state, training=True):\n",
    "        if self.steps_done % self.epsilon_decay == 0:\n",
    "            decay_amount = 0.001\n",
    "            self.current_epsilon = max(self.epsilon_end, self.current_epsilon - decay_amount)\n",
    "        \n",
    "        if training and random.random() < self.current_epsilon:\n",
    "            return random.randint(0, 1)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            state = state.to(self.device)\n",
    "            q_values = self.policy_net(state)\n",
    "            action = q_values.max(1)[1].item()\n",
    "            # Clean up\n",
    "            del q_values\n",
    "            return action\n",
    "    \n",
    "    def optimize_model(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "        \n",
    "        # Sample and move to device\n",
    "        states, actions, rewards, next_states, dones = self.memory.sample(self.batch_size)\n",
    "        states = states.to(self.device)\n",
    "        actions = actions.to(self.device)\n",
    "        rewards = rewards.to(self.device)\n",
    "        next_states = next_states.to(self.device)\n",
    "        dones = dones.to(self.device)\n",
    "        \n",
    "        # Compute loss\n",
    "        with torch.no_grad():\n",
    "            next_actions = self.policy_net(next_states).max(1)[1].unsqueeze(1)\n",
    "            next_q_values = self.target_net(next_states).gather(1, next_actions)\n",
    "            expected_q_values = rewards.unsqueeze(1) + (1 - dones.float().unsqueeze(1)) * self.gamma * next_q_values\n",
    "        \n",
    "        current_q_values = self.policy_net(states).gather(1, actions.unsqueeze(1))\n",
    "        loss = nn.MSELoss()(current_q_values, expected_q_values)\n",
    "        \n",
    "        # Optimize\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.policy_net.parameters(), 10)\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # Clean up\n",
    "        del states, actions, rewards, next_states, dones\n",
    "        del next_actions, next_q_values, expected_q_values, current_q_values\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "    def update_hyperparameters(self):\n",
    "        \"\"\"Update hyperparameters based on the YAML configuration\"\"\"\n",
    "        try:\n",
    "            with open('hyperparameter_updates.yaml', 'r') as file:\n",
    "                config = yaml.safe_load(file)\n",
    "                \n",
    "            if config['status'] == 'Update':\n",
    "                print(\"\\nUpdating hyperparameters...\")\n",
    "                updates = config['updates']\n",
    "                \n",
    "                # Update learning rate\n",
    "                if 'learning_rate' in updates:\n",
    "                    lr_update = updates['learning_rate']\n",
    "                    current_lr = self.optimizer.param_groups[0]['lr']\n",
    "                    \n",
    "                    if lr_update['action'] == 'add':\n",
    "                        new_lr = current_lr + lr_update['value']\n",
    "                    elif lr_update['action'] == 'sub':\n",
    "                        new_lr = current_lr - lr_update['value']\n",
    "                    elif lr_update['action'] == 'mul':\n",
    "                        new_lr = current_lr * lr_update['value']\n",
    "                    \n",
    "                    for param_group in self.optimizer.param_groups:\n",
    "                        param_group['lr'] = new_lr\n",
    "                    print(f\"Learning rate updated: {current_lr:.4f} -> {new_lr:.7f}\")\n",
    "                \n",
    "                # Update current epsilon\n",
    "                if 'epsilon' in updates:\n",
    "                    eps_update = updates['epsilon']\n",
    "                    old_epsilon = self.current_epsilon\n",
    "                    if eps_update['action'] == 'add':\n",
    "                        self.current_epsilon = min(1.0, self.current_epsilon + eps_update['value'])\n",
    "                    elif eps_update['action'] == 'sub':\n",
    "                        self.current_epsilon = max(self.epsilon_end, self.current_epsilon - eps_update['value'])\n",
    "                    print(f\"Current epsilon updated: {old_epsilon:.4f} -> {self.current_epsilon:.4f}\")\n",
    "                \n",
    "                # Update epsilon decay\n",
    "                if 'epsilon_decay' in updates:\n",
    "                    decay_update = updates['epsilon_decay']\n",
    "                    old_decay = self.epsilon_decay\n",
    "                    if decay_update['action'] == 'mul':\n",
    "                        self.epsilon_decay = int(self.epsilon_decay * decay_update['value'])\n",
    "                    print(f\"Epsilon decay updated: {old_decay} -> {self.epsilon_decay}\")\n",
    "                \n",
    "                # Set status to \"Ignore\" after applying updates\n",
    "                config['status'] = 'Ignore'\n",
    "                with open('hyperparameter_updates.yaml', 'w') as file:\n",
    "                    yaml.dump(config, file)\n",
    "                \n",
    "                print(\"Hyperparameter updates complete\\n\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error updating hyperparameters: {str(e)}\")\n",
    "            \n",
    "    def load_model(self, episode_number):\n",
    "        \"\"\"Load a previously saved model and handle architecture changes\"\"\"\n",
    "        model_path = os.path.join('models', f'dqn_episode_{episode_number}.pth')\n",
    "        if os.path.exists(model_path):\n",
    "            checkpoint = torch.load(model_path)\n",
    "            \n",
    "            # Handle the case where we're loading a single-channel model into a four-channel model\n",
    "            old_state_dict = checkpoint['model_state_dict']\n",
    "            new_state_dict = self.policy_net.state_dict()\n",
    "            \n",
    "            # Special handling for the first conv layer\n",
    "            if 'conv_layers.0.weight' in old_state_dict:\n",
    "                old_weights = old_state_dict['conv_layers.0.weight']\n",
    "                if old_weights.size(1) == 1 and new_state_dict['conv_layers.0.weight'].size(1) == 4:\n",
    "                    # Duplicate the single channel weights across all 4 channels\n",
    "                    new_weights = old_weights.repeat(1, 4, 1, 1)\n",
    "                    old_state_dict['conv_layers.0.weight'] = new_weights\n",
    "            \n",
    "            # Load the modified state dict\n",
    "            self.policy_net.load_state_dict(old_state_dict)\n",
    "            self.target_net.load_state_dict(old_state_dict)\n",
    "            self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "            self.steps_done = episode_number * 1000  # Approximate steps based on episode\n",
    "            print(f\"Successfully loaded and adapted model from episode {episode_number}\")\n",
    "            return checkpoint['episode'], checkpoint['reward']\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"No model found at {model_path}\")\n",
    "\n",
    "    def clear_memory(self):\n",
    "        self.memory.clear()\n",
    "        self.frame_stack.clear()\n",
    "        gc.collect()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    def train(self, num_episodes=2000000, start_episode=None, load_existing=True):\n",
    "        os.makedirs('models', exist_ok=True)\n",
    "        os.makedirs('images', exist_ok=True)\n",
    "        \n",
    "        rewards_history = deque(maxlen=100)\n",
    "        best_reward = float('-inf')\n",
    "        \n",
    "        if start_episode is not None and load_existing:\n",
    "            try:\n",
    "                episode_num, last_reward = self.load_model(start_episode)\n",
    "                best_reward = last_reward\n",
    "                start_episode = episode_num\n",
    "                print(\"Continuing training from episode\", start_episode)\n",
    "            except FileNotFoundError:\n",
    "                print(f\"No existing model found for episode {start_episode}, starting fresh\")\n",
    "                start_episode = 0\n",
    "        else:\n",
    "            start_episode = 0\n",
    "        \n",
    "        try:\n",
    "            for episode in range(start_episode, num_episodes):\n",
    "                state, _ = self.env.reset()\n",
    "                frame = preprocess_image(self.env.render())\n",
    "                \n",
    "                self.frame_stack = FrameStack(size=4)\n",
    "                for _ in range(4):\n",
    "                    self.frame_stack.push(frame)\n",
    "                state = self.frame_stack.get_state()\n",
    "                \n",
    "                episode_reward = 0\n",
    "                done = False\n",
    "                \n",
    "                while not done:\n",
    "                    action = self.select_action(state)\n",
    "                    \n",
    "                    skip_reward = 0\n",
    "                    for _ in range(self.frame_skip):\n",
    "                        next_state, reward, done, truncated, _ = self.env.step(action)\n",
    "                        skip_reward += reward\n",
    "                        if done:\n",
    "                            break\n",
    "                    \n",
    "                    next_frame = preprocess_image(self.env.render())\n",
    "                    self.frame_stack.push(next_frame)\n",
    "                    next_state = self.frame_stack.get_state()\n",
    "                    \n",
    "                    self.memory.push(state, action, skip_reward, next_state, done)\n",
    "                    self.optimize_model()\n",
    "                    \n",
    "                    state = next_state\n",
    "                    episode_reward += skip_reward\n",
    "                    self.steps_done += 1\n",
    "                    \n",
    "                    if self.steps_done % self.target_update == 0:\n",
    "                        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "                \n",
    "                rewards_history.append(episode_reward)\n",
    "                avg_reward = np.mean(rewards_history)\n",
    "                \n",
    "                # Periodic cleanup\n",
    "                if episode % 100 == 0:\n",
    "                    gc.collect()\n",
    "                    if torch.cuda.is_available():\n",
    "                        torch.cuda.empty_cache()\n",
    "                \n",
    "                # Save model periodically\n",
    "                if episode % 1000 == 0:\n",
    "                    self.clear_memory()  # Clear memory before saving\n",
    "                    model_path = os.path.join('models', f'dqn_episode_{episode}.pth')\n",
    "                    torch.save({\n",
    "                        'episode': episode,\n",
    "                        'model_state_dict': self.policy_net.state_dict(),\n",
    "                        'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "                        'reward': episode_reward,\n",
    "                    }, model_path)\n",
    "                \n",
    "                if episode_reward > best_reward:\n",
    "                    best_reward = episode_reward\n",
    "                    best_model_path = os.path.join('models', 'dqn_best.pth')\n",
    "                    torch.save({\n",
    "                        'episode': episode,\n",
    "                        'model_state_dict': self.policy_net.state_dict(),\n",
    "                        'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "                        'reward': episode_reward,\n",
    "                    }, best_model_path)\n",
    "                    print(f\"New best model saved at episode {episode} with reward {episode_reward:.2f}\")\n",
    "                \n",
    "                if (episode + 1) % 100 == 0:\n",
    "                    print(f\"Episode {episode + 1}/{num_episodes} - \"\n",
    "                          f\"Reward: {episode_reward:.2f} - \"\n",
    "                          f\"Average Reward (100 ep): {avg_reward:.2f} - \"\n",
    "                          f\"Best Reward: {best_reward:.2f} - \"\n",
    "                          f\"Epsilon: {self.current_epsilon:.7f} - \"\n",
    "                          f\"Learning Rate: {self.optimizer.param_groups[0]['lr']:.7f}\")\n",
    "                \n",
    "                self.update_hyperparameters()\n",
    "                \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\nTraining interrupted. Cleaning up...\")\n",
    "            self.clear_memory()\n",
    "        \n",
    "        finally:\n",
    "            self.clear_memory()\n",
    "\n",
    "def main():\n",
    "    env = gym.make(\"FlappyBird-v0\", render_mode=\"rgb_array\", use_lidar=False)\n",
    "    agent = DQNAgent(env)\n",
    "    \n",
    "    try:\n",
    "        agent.train(start_episode=10000)\n",
    "    finally:\n",
    "        env.close()\n",
    "        gc.collect()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mihai\\AppData\\Local\\Temp\\ipykernel_36364\\3957765632.py:244: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(model_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded and adapted model from episode 10000\n",
      "Continuing training from episode 10000\n",
      "New best model saved at episode 10005 with reward 17.90\n",
      "New best model saved at episode 10047 with reward 20.10\n",
      "Episode 10100/2000000 - Reward: 8.70 - Average Reward (100 ep): 6.88 - Best Reward: 20.10 - Epsilon: 0.1920000 - Learning Rate: 0.0001250\n",
      "New best model saved at episode 10138 with reward 23.20\n",
      "Episode 10200/2000000 - Reward: 3.90 - Average Reward (100 ep): 5.52 - Best Reward: 23.20 - Epsilon: 0.1840000 - Learning Rate: 0.0001250\n",
      "New best model saved at episode 10213 with reward 26.90\n",
      "New best model saved at episode 10263 with reward 29.70\n",
      "Episode 10300/2000000 - Reward: 22.40 - Average Reward (100 ep): 6.27 - Best Reward: 29.70 - Epsilon: 0.1750000 - Learning Rate: 0.0001250\n",
      "Episode 10400/2000000 - Reward: 3.90 - Average Reward (100 ep): 6.48 - Best Reward: 29.70 - Epsilon: 0.1670000 - Learning Rate: 0.0001250\n",
      "Episode 10500/2000000 - Reward: 3.30 - Average Reward (100 ep): 6.46 - Best Reward: 29.70 - Epsilon: 0.1590000 - Learning Rate: 0.0001250\n",
      "Episode 10600/2000000 - Reward: 9.40 - Average Reward (100 ep): 6.59 - Best Reward: 29.70 - Epsilon: 0.1510000 - Learning Rate: 0.0001250\n",
      "New best model saved at episode 10618 with reward 34.50\n",
      "Episode 10700/2000000 - Reward: 3.90 - Average Reward (100 ep): 7.35 - Best Reward: 34.50 - Epsilon: 0.1410000 - Learning Rate: 0.0001250\n",
      "Episode 10800/2000000 - Reward: 8.70 - Average Reward (100 ep): 7.20 - Best Reward: 34.50 - Epsilon: 0.1320000 - Learning Rate: 0.0001250\n",
      "Episode 10900/2000000 - Reward: 6.70 - Average Reward (100 ep): 7.18 - Best Reward: 34.50 - Epsilon: 0.1230000 - Learning Rate: 0.0001250\n",
      "New best model saved at episode 10940 with reward 36.80\n",
      "Episode 11000/2000000 - Reward: 3.90 - Average Reward (100 ep): 8.47 - Best Reward: 36.80 - Epsilon: 0.1130000 - Learning Rate: 0.0001250\n",
      "Episode 11100/2000000 - Reward: 3.90 - Average Reward (100 ep): 8.00 - Best Reward: 36.80 - Epsilon: 0.1040000 - Learning Rate: 0.0001250\n",
      "New best model saved at episode 11129 with reward 37.00\n",
      "New best model saved at episode 11159 with reward 46.70\n",
      "Episode 11200/2000000 - Reward: 24.60 - Average Reward (100 ep): 10.42 - Best Reward: 46.70 - Epsilon: 0.0920000 - Learning Rate: 0.0001250\n",
      "New best model saved at episode 11228 with reward 59.90\n",
      "Episode 11300/2000000 - Reward: 10.90 - Average Reward (100 ep): 12.00 - Best Reward: 59.90 - Epsilon: 0.0790000 - Learning Rate: 0.0001250\n",
      "Episode 11400/2000000 - Reward: 13.50 - Average Reward (100 ep): 10.36 - Best Reward: 59.90 - Epsilon: 0.0670000 - Learning Rate: 0.0001250\n",
      "Episode 11500/2000000 - Reward: 11.20 - Average Reward (100 ep): 13.06 - Best Reward: 59.90 - Epsilon: 0.0530000 - Learning Rate: 0.0001250\n",
      "New best model saved at episode 11542 with reward 76.80\n",
      "Episode 11600/2000000 - Reward: 17.90 - Average Reward (100 ep): 15.98 - Best Reward: 76.80 - Epsilon: 0.0500000 - Learning Rate: 0.0001250\n",
      "Episode 11700/2000000 - Reward: 25.00 - Average Reward (100 ep): 14.62 - Best Reward: 76.80 - Epsilon: 0.0500000 - Learning Rate: 0.0001250\n",
      "Episode 11800/2000000 - Reward: 6.10 - Average Reward (100 ep): 15.48 - Best Reward: 76.80 - Epsilon: 0.0500000 - Learning Rate: 0.0001250\n",
      "Episode 11900/2000000 - Reward: 9.20 - Average Reward (100 ep): 16.28 - Best Reward: 76.80 - Epsilon: 0.0500000 - Learning Rate: 0.0001250\n",
      "Episode 12000/2000000 - Reward: 9.10 - Average Reward (100 ep): 16.95 - Best Reward: 76.80 - Epsilon: 0.0500000 - Learning Rate: 0.0001250\n",
      "Episode 12100/2000000 - Reward: 4.80 - Average Reward (100 ep): 15.15 - Best Reward: 76.80 - Epsilon: 0.0500000 - Learning Rate: 0.0001250\n",
      "Episode 12200/2000000 - Reward: 4.70 - Average Reward (100 ep): 14.12 - Best Reward: 76.80 - Epsilon: 0.0500000 - Learning Rate: 0.0001250\n",
      "\n",
      "Updating hyperparameters...\n",
      "Learning rate updated: 0.0001 -> 0.0000625\n",
      "Current epsilon updated: 0.0500 -> 0.0500\n",
      "Epsilon decay updated: 300 -> 300\n",
      "Hyperparameter updates complete\n",
      "\n",
      "Episode 12300/2000000 - Reward: 15.40 - Average Reward (100 ep): 16.62 - Best Reward: 76.80 - Epsilon: 0.0500000 - Learning Rate: 0.0000625\n",
      "Episode 12400/2000000 - Reward: 26.90 - Average Reward (100 ep): 15.67 - Best Reward: 76.80 - Epsilon: 0.0500000 - Learning Rate: 0.0000625\n",
      "Episode 12500/2000000 - Reward: 14.00 - Average Reward (100 ep): 14.69 - Best Reward: 76.80 - Epsilon: 0.0500000 - Learning Rate: 0.0000625\n",
      "Episode 12600/2000000 - Reward: 13.20 - Average Reward (100 ep): 16.86 - Best Reward: 76.80 - Epsilon: 0.0500000 - Learning Rate: 0.0000625\n",
      "New best model saved at episode 12636 with reward 85.80\n",
      "Episode 12700/2000000 - Reward: 6.20 - Average Reward (100 ep): 17.27 - Best Reward: 85.80 - Epsilon: 0.0500000 - Learning Rate: 0.0000625\n",
      "Episode 12800/2000000 - Reward: 17.90 - Average Reward (100 ep): 17.46 - Best Reward: 85.80 - Epsilon: 0.0500000 - Learning Rate: 0.0000625\n",
      "Episode 12900/2000000 - Reward: 26.90 - Average Reward (100 ep): 17.93 - Best Reward: 85.80 - Epsilon: 0.0500000 - Learning Rate: 0.0000625\n",
      "Episode 13000/2000000 - Reward: 24.80 - Average Reward (100 ep): 18.74 - Best Reward: 85.80 - Epsilon: 0.0500000 - Learning Rate: 0.0000625\n",
      "Episode 13100/2000000 - Reward: 6.80 - Average Reward (100 ep): 14.86 - Best Reward: 85.80 - Epsilon: 0.0500000 - Learning Rate: 0.0000625\n",
      "Episode 13200/2000000 - Reward: -0.30 - Average Reward (100 ep): 16.17 - Best Reward: 85.80 - Epsilon: 0.0500000 - Learning Rate: 0.0000625\n",
      "Episode 13300/2000000 - Reward: 4.90 - Average Reward (100 ep): 16.23 - Best Reward: 85.80 - Epsilon: 0.0500000 - Learning Rate: 0.0000625\n",
      "New best model saved at episode 13340 with reward 87.90\n",
      "Episode 13400/2000000 - Reward: 6.60 - Average Reward (100 ep): 16.63 - Best Reward: 87.90 - Epsilon: 0.0500000 - Learning Rate: 0.0000625\n",
      "New best model saved at episode 13417 with reward 88.90\n",
      "Episode 13500/2000000 - Reward: 4.50 - Average Reward (100 ep): 17.58 - Best Reward: 88.90 - Epsilon: 0.0500000 - Learning Rate: 0.0000625\n",
      "Episode 13600/2000000 - Reward: 15.70 - Average Reward (100 ep): 16.32 - Best Reward: 88.90 - Epsilon: 0.0500000 - Learning Rate: 0.0000625\n",
      "New best model saved at episode 13624 with reward 113.30\n",
      "Episode 13700/2000000 - Reward: 15.30 - Average Reward (100 ep): 16.70 - Best Reward: 113.30 - Epsilon: 0.0500000 - Learning Rate: 0.0000625\n",
      "Episode 13800/2000000 - Reward: 8.40 - Average Reward (100 ep): 18.07 - Best Reward: 113.30 - Epsilon: 0.0500000 - Learning Rate: 0.0000625\n",
      "Episode 13900/2000000 - Reward: 60.10 - Average Reward (100 ep): 14.34 - Best Reward: 113.30 - Epsilon: 0.0500000 - Learning Rate: 0.0000625\n",
      "Episode 14000/2000000 - Reward: 23.60 - Average Reward (100 ep): 17.60 - Best Reward: 113.30 - Epsilon: 0.0500000 - Learning Rate: 0.0000625\n",
      "Episode 14100/2000000 - Reward: 8.40 - Average Reward (100 ep): 15.96 - Best Reward: 113.30 - Epsilon: 0.0500000 - Learning Rate: 0.0000625\n",
      "Episode 14200/2000000 - Reward: 3.90 - Average Reward (100 ep): 16.11 - Best Reward: 113.30 - Epsilon: 0.0500000 - Learning Rate: 0.0000625\n",
      "Episode 14300/2000000 - Reward: 25.30 - Average Reward (100 ep): 18.24 - Best Reward: 113.30 - Epsilon: 0.0500000 - Learning Rate: 0.0000625\n",
      "Episode 14400/2000000 - Reward: 15.40 - Average Reward (100 ep): 17.86 - Best Reward: 113.30 - Epsilon: 0.0500000 - Learning Rate: 0.0000625\n",
      "Episode 14500/2000000 - Reward: 8.40 - Average Reward (100 ep): 16.98 - Best Reward: 113.30 - Epsilon: 0.0500000 - Learning Rate: 0.0000625\n",
      "Episode 14600/2000000 - Reward: 17.90 - Average Reward (100 ep): 17.33 - Best Reward: 113.30 - Epsilon: 0.0500000 - Learning Rate: 0.0000625\n",
      "Episode 14700/2000000 - Reward: 38.60 - Average Reward (100 ep): 20.84 - Best Reward: 113.30 - Epsilon: 0.0500000 - Learning Rate: 0.0000625\n",
      "Episode 14800/2000000 - Reward: 4.80 - Average Reward (100 ep): 19.46 - Best Reward: 113.30 - Epsilon: 0.0500000 - Learning Rate: 0.0000625\n",
      "Episode 14900/2000000 - Reward: 4.70 - Average Reward (100 ep): 16.49 - Best Reward: 113.30 - Epsilon: 0.0500000 - Learning Rate: 0.0000625\n",
      "New best model saved at episode 14998 with reward 116.60\n",
      "Episode 15000/2000000 - Reward: 8.60 - Average Reward (100 ep): 18.64 - Best Reward: 116.60 - Epsilon: 0.0500000 - Learning Rate: 0.0000625\n",
      "Episode 15100/2000000 - Reward: 36.40 - Average Reward (100 ep): 17.11 - Best Reward: 116.60 - Epsilon: 0.0500000 - Learning Rate: 0.0000625\n",
      "Episode 15200/2000000 - Reward: 36.40 - Average Reward (100 ep): 17.61 - Best Reward: 116.60 - Epsilon: 0.0500000 - Learning Rate: 0.0000625\n",
      "Episode 15300/2000000 - Reward: 38.70 - Average Reward (100 ep): 16.76 - Best Reward: 116.60 - Epsilon: 0.0500000 - Learning Rate: 0.0000625\n",
      "Episode 15400/2000000 - Reward: 13.10 - Average Reward (100 ep): 14.38 - Best Reward: 116.60 - Epsilon: 0.0500000 - Learning Rate: 0.0000625\n",
      "Episode 15500/2000000 - Reward: 22.40 - Average Reward (100 ep): 17.78 - Best Reward: 116.60 - Epsilon: 0.0500000 - Learning Rate: 0.0000625\n",
      "Episode 15600/2000000 - Reward: 13.80 - Average Reward (100 ep): 16.84 - Best Reward: 116.60 - Epsilon: 0.0500000 - Learning Rate: 0.0000625\n",
      "Episode 15700/2000000 - Reward: 8.60 - Average Reward (100 ep): 18.92 - Best Reward: 116.60 - Epsilon: 0.0500000 - Learning Rate: 0.0000625\n",
      "Episode 15800/2000000 - Reward: 15.70 - Average Reward (100 ep): 17.11 - Best Reward: 116.60 - Epsilon: 0.0500000 - Learning Rate: 0.0000625\n",
      "Episode 15900/2000000 - Reward: 32.30 - Average Reward (100 ep): 16.99 - Best Reward: 116.60 - Epsilon: 0.0500000 - Learning Rate: 0.0000625\n",
      "Episode 16000/2000000 - Reward: 20.10 - Average Reward (100 ep): 17.18 - Best Reward: 116.60 - Epsilon: 0.0500000 - Learning Rate: 0.0000625\n",
      "Episode 16100/2000000 - Reward: 17.90 - Average Reward (100 ep): 19.43 - Best Reward: 116.60 - Epsilon: 0.0500000 - Learning Rate: 0.0000625\n",
      "Episode 16200/2000000 - Reward: 34.30 - Average Reward (100 ep): 17.71 - Best Reward: 116.60 - Epsilon: 0.0500000 - Learning Rate: 0.0000625\n",
      "Episode 16300/2000000 - Reward: 12.90 - Average Reward (100 ep): 18.17 - Best Reward: 116.60 - Epsilon: 0.0500000 - Learning Rate: 0.0000625\n",
      "Episode 16400/2000000 - Reward: 18.80 - Average Reward (100 ep): 20.50 - Best Reward: 116.60 - Epsilon: 0.0500000 - Learning Rate: 0.0000625\n",
      "Episode 16500/2000000 - Reward: 8.40 - Average Reward (100 ep): 17.53 - Best Reward: 116.60 - Epsilon: 0.0500000 - Learning Rate: 0.0000625\n",
      "Episode 16600/2000000 - Reward: 9.00 - Average Reward (100 ep): 16.09 - Best Reward: 116.60 - Epsilon: 0.0500000 - Learning Rate: 0.0000625\n",
      "Episode 16700/2000000 - Reward: 15.80 - Average Reward (100 ep): 15.75 - Best Reward: 116.60 - Epsilon: 0.0500000 - Learning Rate: 0.0000625\n",
      "Episode 16800/2000000 - Reward: 41.80 - Average Reward (100 ep): 18.49 - Best Reward: 116.60 - Epsilon: 0.0500000 - Learning Rate: 0.0000625\n",
      "Episode 16900/2000000 - Reward: 3.90 - Average Reward (100 ep): 16.60 - Best Reward: 116.60 - Epsilon: 0.0500000 - Learning Rate: 0.0000625\n",
      "Episode 17000/2000000 - Reward: 9.50 - Average Reward (100 ep): 19.09 - Best Reward: 116.60 - Epsilon: 0.0500000 - Learning Rate: 0.0000625\n",
      "Episode 17100/2000000 - Reward: 18.40 - Average Reward (100 ep): 17.14 - Best Reward: 116.60 - Epsilon: 0.0500000 - Learning Rate: 0.0000625\n",
      "Episode 17200/2000000 - Reward: 34.20 - Average Reward (100 ep): 16.86 - Best Reward: 116.60 - Epsilon: 0.0500000 - Learning Rate: 0.0000625\n",
      "Episode 17300/2000000 - Reward: 10.60 - Average Reward (100 ep): 15.45 - Best Reward: 116.60 - Epsilon: 0.0500000 - Learning Rate: 0.0000625\n",
      "Episode 17400/2000000 - Reward: 13.70 - Average Reward (100 ep): 20.05 - Best Reward: 116.60 - Epsilon: 0.0500000 - Learning Rate: 0.0000625\n",
      "Episode 17500/2000000 - Reward: 26.90 - Average Reward (100 ep): 16.46 - Best Reward: 116.60 - Epsilon: 0.0500000 - Learning Rate: 0.0000625\n",
      "Episode 17600/2000000 - Reward: 3.90 - Average Reward (100 ep): 18.63 - Best Reward: 116.60 - Epsilon: 0.0500000 - Learning Rate: 0.0000625\n",
      "Episode 17700/2000000 - Reward: 45.90 - Average Reward (100 ep): 21.06 - Best Reward: 116.60 - Epsilon: 0.0500000 - Learning Rate: 0.0000625\n",
      "Episode 17800/2000000 - Reward: 43.50 - Average Reward (100 ep): 19.89 - Best Reward: 116.60 - Epsilon: 0.0500000 - Learning Rate: 0.0000625\n",
      "Episode 17900/2000000 - Reward: 23.40 - Average Reward (100 ep): 16.03 - Best Reward: 116.60 - Epsilon: 0.0500000 - Learning Rate: 0.0000625\n",
      "Episode 18000/2000000 - Reward: 0.30 - Average Reward (100 ep): 16.97 - Best Reward: 116.60 - Epsilon: 0.0500000 - Learning Rate: 0.0000625\n",
      "Episode 18100/2000000 - Reward: 12.90 - Average Reward (100 ep): 16.70 - Best Reward: 116.60 - Epsilon: 0.0500000 - Learning Rate: 0.0000625\n",
      "Episode 18200/2000000 - Reward: 40.90 - Average Reward (100 ep): 17.47 - Best Reward: 116.60 - Epsilon: 0.0500000 - Learning Rate: 0.0000625\n",
      "Episode 18300/2000000 - Reward: 13.60 - Average Reward (100 ep): 20.60 - Best Reward: 116.60 - Epsilon: 0.0500000 - Learning Rate: 0.0000625\n",
      "Episode 18400/2000000 - Reward: 3.90 - Average Reward (100 ep): 16.68 - Best Reward: 116.60 - Epsilon: 0.0500000 - Learning Rate: 0.0000625\n",
      "Episode 18500/2000000 - Reward: 3.90 - Average Reward (100 ep): 16.26 - Best Reward: 116.60 - Epsilon: 0.0500000 - Learning Rate: 0.0000625\n",
      "Episode 18600/2000000 - Reward: 13.40 - Average Reward (100 ep): 17.01 - Best Reward: 116.60 - Epsilon: 0.0500000 - Learning Rate: 0.0000625\n",
      "Episode 18700/2000000 - Reward: 18.80 - Average Reward (100 ep): 14.79 - Best Reward: 116.60 - Epsilon: 0.0500000 - Learning Rate: 0.0000625\n",
      "Episode 18800/2000000 - Reward: 4.00 - Average Reward (100 ep): 19.04 - Best Reward: 116.60 - Epsilon: 0.0500000 - Learning Rate: 0.0000625\n",
      "Episode 18900/2000000 - Reward: 4.30 - Average Reward (100 ep): 14.32 - Best Reward: 116.60 - Epsilon: 0.0500000 - Learning Rate: 0.0000625\n",
      "Episode 19000/2000000 - Reward: 50.70 - Average Reward (100 ep): 18.66 - Best Reward: 116.60 - Epsilon: 0.0500000 - Learning Rate: 0.0000625\n",
      "Episode 19100/2000000 - Reward: 12.90 - Average Reward (100 ep): 17.20 - Best Reward: 116.60 - Epsilon: 0.0500000 - Learning Rate: 0.0000625\n",
      "Episode 19200/2000000 - Reward: 50.40 - Average Reward (100 ep): 17.51 - Best Reward: 116.60 - Epsilon: 0.0500000 - Learning Rate: 0.0000625\n",
      "Episode 19300/2000000 - Reward: 8.80 - Average Reward (100 ep): 15.88 - Best Reward: 116.60 - Epsilon: 0.0500000 - Learning Rate: 0.0000625\n",
      "Episode 19400/2000000 - Reward: 9.40 - Average Reward (100 ep): 16.55 - Best Reward: 116.60 - Epsilon: 0.0500000 - Learning Rate: 0.0000625\n",
      "Episode 19500/2000000 - Reward: 3.90 - Average Reward (100 ep): 18.14 - Best Reward: 116.60 - Epsilon: 0.0500000 - Learning Rate: 0.0000625\n",
      "Episode 19600/2000000 - Reward: 18.80 - Average Reward (100 ep): 17.54 - Best Reward: 116.60 - Epsilon: 0.0500000 - Learning Rate: 0.0000625\n",
      "Episode 19700/2000000 - Reward: 10.50 - Average Reward (100 ep): 17.90 - Best Reward: 116.60 - Epsilon: 0.0500000 - Learning Rate: 0.0000625\n",
      "Episode 19800/2000000 - Reward: 13.70 - Average Reward (100 ep): 16.65 - Best Reward: 116.60 - Epsilon: 0.0500000 - Learning Rate: 0.0000625\n",
      "Episode 19900/2000000 - Reward: 31.90 - Average Reward (100 ep): 15.53 - Best Reward: 116.60 - Epsilon: 0.0500000 - Learning Rate: 0.0000625\n",
      "Episode 20000/2000000 - Reward: 31.90 - Average Reward (100 ep): 16.99 - Best Reward: 116.60 - Epsilon: 0.0500000 - Learning Rate: 0.0000625\n",
      "Episode 20100/2000000 - Reward: 3.90 - Average Reward (100 ep): 15.61 - Best Reward: 116.60 - Epsilon: 0.0500000 - Learning Rate: 0.0000625\n",
      "Episode 20200/2000000 - Reward: 6.30 - Average Reward (100 ep): 14.98 - Best Reward: 116.60 - Epsilon: 0.0500000 - Learning Rate: 0.0000625\n",
      "Episode 20300/2000000 - Reward: 17.90 - Average Reward (100 ep): 15.45 - Best Reward: 116.60 - Epsilon: 0.0500000 - Learning Rate: 0.0000625\n",
      "\n",
      "Training interrupted. Cleaning up...\n"
     ]
    }
   ],
   "execution_count": 3
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
