{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-22T23:32:49.993618Z",
     "start_time": "2024-12-22T23:26:30.889626Z"
    }
   },
   "source": [
    "import gymnasium as gym\n",
    "import flappy_bird_gymnasium\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import random\n",
    "from PIL import Image\n",
    "import os\n",
    "from datetime import datetime\n",
    "import yaml\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_channels=1, n_actions=2):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(3136, 512),  # 3136 = 64 * 7 * 7\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, n_actions)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc_layers(x)\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity=100000):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "        \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        state, action, reward, next_state, done = zip(*random.sample(self.buffer, batch_size))\n",
    "        return (torch.cat(state), \n",
    "                torch.tensor(action), \n",
    "                torch.tensor(reward), \n",
    "                torch.cat(next_state),\n",
    "                torch.tensor(done))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "def preprocess_image(image):\n",
    "    \"\"\"\n",
    "    Preprocess the image by converting it to grayscale and resizing it.\n",
    "    Returns the image as a PyTorch tensor.\n",
    "    \"\"\"\n",
    "    # Convert to PIL Image for processing\n",
    "    pil_image = Image.fromarray(image)\n",
    "    \n",
    "    # Convert to grayscale\n",
    "    grayscale_image = pil_image.convert(\"L\")\n",
    "    \n",
    "    # Resize the image to a fixed size (84x84)\n",
    "    resized_image = grayscale_image.resize((84, 84))\n",
    "    \n",
    "    # Save the preprocessed image with 0.01 probability\n",
    "    # if random.random() < 0.01:\n",
    "    #     resized_image.save(os.path.join('images', f'preprocessed_image_{datetime.now().strftime(\"%Y%m%d%H%M%S\")}.png'))\n",
    "    \n",
    "    # Convert to numpy array and normalize\n",
    "    preprocessed_image = np.array(resized_image) / 255.0\n",
    "    \n",
    "    # Convert to PyTorch tensor and add batch and channel dimensions\n",
    "    tensor_image = torch.FloatTensor(preprocessed_image).unsqueeze(0).unsqueeze(0)\n",
    "    \n",
    "    return tensor_image\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.policy_net = DQN().to(self.device)\n",
    "        self.target_net = DQN().to(self.device)\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=0.00015)\n",
    "        self.memory = ReplayBuffer()\n",
    "        \n",
    "        self.batch_size = 32\n",
    "        self.gamma = 0.99\n",
    "        self.epsilon_start = 0.05\n",
    "        self.epsilon_end = 0.01\n",
    "        self.epsilon_decay = 50000\n",
    "        self.current_epsilon = self.epsilon_start\n",
    "        self.target_update = 10000\n",
    "        self.frame_skip = 4\n",
    "        \n",
    "        self.steps_done = 0\n",
    "        \n",
    "    def select_action(self, state, training=True):\n",
    "        if self.steps_done % self.epsilon_decay == 0:\n",
    "            decay_amount = 0.001\n",
    "            self.current_epsilon = max(self.epsilon_end, self.current_epsilon - decay_amount)\n",
    "        \n",
    "        if training and random.random() < self.current_epsilon:\n",
    "            return random.randint(0, 1)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            state = state.to(self.device)\n",
    "            q_values = self.policy_net(state)\n",
    "            return q_values.max(1)[1].item()\n",
    "    \n",
    "    def optimize_model(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "        \n",
    "        states, actions, rewards, next_states, dones = self.memory.sample(self.batch_size)\n",
    "        states = states.to(self.device)\n",
    "        actions = actions.to(self.device)\n",
    "        rewards = rewards.to(self.device)\n",
    "        next_states = next_states.to(self.device)\n",
    "        dones = dones.to(self.device)\n",
    "        \n",
    "        current_q_values = self.policy_net(states).gather(1, actions.unsqueeze(1))\n",
    "        next_q_values = self.target_net(next_states).max(1)[0].detach()\n",
    "        expected_q_values = rewards + (1 - dones.float()) * self.gamma * next_q_values\n",
    "        \n",
    "        loss = nn.MSELoss()(current_q_values, expected_q_values.unsqueeze(1))\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.policy_net.parameters(), 10)\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def update_hyperparameters(self):\n",
    "        \"\"\"Update hyperparameters based on the YAML configuration\"\"\"\n",
    "        try:\n",
    "            with open('hyperparameter_updates.yaml', 'r') as file:\n",
    "                config = yaml.safe_load(file)\n",
    "                \n",
    "            if config['status'] == 'Update':\n",
    "                print(\"\\nUpdating hyperparameters...\")\n",
    "                updates = config['updates']\n",
    "                \n",
    "                # Update learning rate\n",
    "                if 'learning_rate' in updates:\n",
    "                    lr_update = updates['learning_rate']\n",
    "                    current_lr = self.optimizer.param_groups[0]['lr']\n",
    "                    \n",
    "                    if lr_update['action'] == 'add':\n",
    "                        new_lr = current_lr + lr_update['value']\n",
    "                    elif lr_update['action'] == 'sub':\n",
    "                        new_lr = current_lr - lr_update['value']\n",
    "                    elif lr_update['action'] == 'mul':\n",
    "                        new_lr = current_lr * lr_update['value']\n",
    "                    \n",
    "                    for param_group in self.optimizer.param_groups:\n",
    "                        param_group['lr'] = new_lr\n",
    "                    print(f\"Learning rate updated: {current_lr:.4f} -> {new_lr:.7f}\")\n",
    "                \n",
    "                # Update current epsilon\n",
    "                if 'epsilon' in updates:\n",
    "                    eps_update = updates['epsilon']\n",
    "                    old_epsilon = self.current_epsilon\n",
    "                    if eps_update['action'] == 'add':\n",
    "                        self.current_epsilon = min(1.0, self.current_epsilon + eps_update['value'])\n",
    "                    elif eps_update['action'] == 'sub':\n",
    "                        self.current_epsilon = max(self.epsilon_end, self.current_epsilon - eps_update['value'])\n",
    "                    print(f\"Current epsilon updated: {old_epsilon:.4f} -> {self.current_epsilon:.4f}\")\n",
    "                \n",
    "                # Update epsilon decay\n",
    "                if 'epsilon_decay' in updates:\n",
    "                    decay_update = updates['epsilon_decay']\n",
    "                    old_decay = self.epsilon_decay\n",
    "                    if decay_update['action'] == 'mul':\n",
    "                        self.epsilon_decay = int(self.epsilon_decay * decay_update['value'])\n",
    "                    print(f\"Epsilon decay updated: {old_decay} -> {self.epsilon_decay}\")\n",
    "                \n",
    "                # Set status to \"Ignore\" after applying updates\n",
    "                config['status'] = 'Ignore'\n",
    "                with open('hyperparameter_updates.yaml', 'w') as file:\n",
    "                    yaml.dump(config, file)\n",
    "                \n",
    "                print(\"Hyperparameter updates complete\\n\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error updating hyperparameters: {str(e)}\")\n",
    "    \n",
    "    def load_model(self, episode_number):\n",
    "        \"\"\"Load a previously saved model\"\"\"\n",
    "        model_path = os.path.join('models', f'dqn_episode_{episode_number}.pth')\n",
    "        if os.path.exists(model_path):\n",
    "            checkpoint = torch.load(model_path)\n",
    "            self.policy_net.load_state_dict(checkpoint['model_state_dict'])\n",
    "            self.target_net.load_state_dict(checkpoint['model_state_dict'])\n",
    "            self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "            self.steps_done = episode_number * 1000  # Approximate steps based on episode\n",
    "            print(f\"Successfully loaded model from episode {episode_number}\")\n",
    "            return checkpoint['episode'], checkpoint['reward']\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"No model found at {model_path}\")\n",
    "\n",
    "    def train(self, num_episodes=2000000, start_episode=None):\n",
    "        \"\"\"Modified train method with start_episode parameter\"\"\"\n",
    "        os.makedirs('models', exist_ok=True)\n",
    "        os.makedirs('images', exist_ok=True)\n",
    "        \n",
    "        rewards_history = deque(maxlen=100)\n",
    "        best_reward = float('-inf')\n",
    "        \n",
    "        # Load previous model if start_episode is specified\n",
    "        if start_episode is not None:\n",
    "            episode_num, last_reward = self.load_model(start_episode)\n",
    "            best_reward = last_reward\n",
    "            start_episode = episode_num\n",
    "        else:\n",
    "            start_episode = 0\n",
    "        \n",
    "        for episode in range(start_episode, num_episodes):\n",
    "            state, _ = self.env.reset()\n",
    "            state = preprocess_image(self.env.render())\n",
    "            episode_reward = 0\n",
    "            done = False\n",
    "            \n",
    "            while not done:\n",
    "                action = self.select_action(state)\n",
    "                \n",
    "                # Frame skipping\n",
    "                skip_reward = 0\n",
    "                for _ in range(self.frame_skip):\n",
    "                    next_state, reward, done, truncated, _ = self.env.step(action)\n",
    "                    skip_reward += reward\n",
    "                    if done:\n",
    "                        break\n",
    "                \n",
    "                next_state = preprocess_image(self.env.render())\n",
    "                \n",
    "                # Store transition in memory\n",
    "                self.memory.push(state, action, skip_reward, next_state, done)\n",
    "                \n",
    "                # Optimize model\n",
    "                self.optimize_model()\n",
    "                \n",
    "                state = next_state\n",
    "                episode_reward += skip_reward\n",
    "                self.steps_done += 1\n",
    "                \n",
    "                # Update target network\n",
    "                if self.steps_done % self.target_update == 0:\n",
    "                    self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "            \n",
    "            rewards_history.append(episode_reward)\n",
    "            avg_reward = np.mean(rewards_history)\n",
    "            \n",
    "            # Save model periodically\n",
    "            if episode % 1000 == 0:\n",
    "                model_path = os.path.join('models', f'dqn_episode_{episode}.pth')\n",
    "                torch.save({\n",
    "                    'episode': episode,\n",
    "                    'model_state_dict': self.policy_net.state_dict(),\n",
    "                    'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "                    'reward': episode_reward,\n",
    "                }, model_path)\n",
    "                print(f\"Model saved to {model_path}\")\n",
    "            \n",
    "            # Save model when we achieve best episode reward\n",
    "            if episode_reward > best_reward:\n",
    "                best_reward = episode_reward\n",
    "                best_model_path = os.path.join('models', 'dqn_best.pth')\n",
    "                torch.save({\n",
    "                    'episode': episode,\n",
    "                    'model_state_dict': self.policy_net.state_dict(),\n",
    "                    'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "                    'reward': episode_reward,\n",
    "                }, best_model_path)\n",
    "                print(f\"New best reward achieved: {best_reward:.2f}\")\n",
    "            \n",
    "            if (episode + 1) % 100 == 0:\n",
    "                print(f\"Episode {episode + 1}/{num_episodes} - \"\n",
    "                      f\"Reward: {episode_reward:.2f} - \"\n",
    "                      f\"Average Reward (100 ep): {avg_reward:.2f} - \"\n",
    "                      f\"Best Reward: {best_reward:.2f} - \"\n",
    "                      f\"Epsilon: {self.current_epsilon:.7f} - \"\n",
    "                      f\"Learning Rate: {self.optimizer.param_groups[0]['lr']:.7f}\")\n",
    "                self.update_hyperparameters()\n",
    "\n",
    "def main():\n",
    "    env = gym.make(\"FlappyBird-v0\", render_mode=\"rgb_array\", use_lidar=False)\n",
    "    agent = DQNAgent(env)\n",
    "    # Start training from episode 20000\n",
    "    agent.train(start_episode=35000)\n",
    "    env.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mihai\\AppData\\Local\\Temp\\ipykernel_29152\\2346595977.py:196: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(model_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded model from episode 35000\n",
      "Model saved to models\\dqn_episode_35000.pth\n",
      "New best reward achieved: 15.70\n",
      "New best reward achieved: 17.90\n",
      "New best reward achieved: 23.40\n",
      "New best reward achieved: 25.10\n",
      "Episode 35100/2000000 - Reward: 10.60 - Average Reward (100 ep): 10.10 - Best Reward: 25.10 - Epsilon: 0.0490000 - Learning Rate: 0.0002500\n",
      "New best reward achieved: 27.60\n",
      "Episode 35200/2000000 - Reward: 19.00 - Average Reward (100 ep): 10.50 - Best Reward: 27.60 - Epsilon: 0.0490000 - Learning Rate: 0.0002500\n",
      "New best reward achieved: 36.80\n",
      "Episode 35300/2000000 - Reward: 6.10 - Average Reward (100 ep): 10.00 - Best Reward: 36.80 - Epsilon: 0.0490000 - Learning Rate: 0.0002500\n",
      "Episode 35400/2000000 - Reward: 8.40 - Average Reward (100 ep): 9.99 - Best Reward: 36.80 - Epsilon: 0.0490000 - Learning Rate: 0.0002500\n",
      "Episode 35500/2000000 - Reward: 3.90 - Average Reward (100 ep): 11.17 - Best Reward: 36.80 - Epsilon: 0.0490000 - Learning Rate: 0.0002500\n",
      "Episode 35600/2000000 - Reward: 12.90 - Average Reward (100 ep): 10.57 - Best Reward: 36.80 - Epsilon: 0.0490000 - Learning Rate: 0.0002500\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 298\u001B[0m\n\u001B[0;32m    295\u001B[0m     env\u001B[38;5;241m.\u001B[39mclose()\n\u001B[0;32m    297\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;18m__name__\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m__main__\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m--> 298\u001B[0m     \u001B[43mmain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[1], line 294\u001B[0m, in \u001B[0;36mmain\u001B[1;34m()\u001B[0m\n\u001B[0;32m    292\u001B[0m agent \u001B[38;5;241m=\u001B[39m DQNAgent(env)\n\u001B[0;32m    293\u001B[0m \u001B[38;5;66;03m# Start training from episode 20000\u001B[39;00m\n\u001B[1;32m--> 294\u001B[0m \u001B[43magent\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstart_episode\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m35000\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m    295\u001B[0m env\u001B[38;5;241m.\u001B[39mclose()\n",
      "Cell \u001B[1;32mIn[1], line 245\u001B[0m, in \u001B[0;36mDQNAgent.train\u001B[1;34m(self, num_episodes, start_episode)\u001B[0m\n\u001B[0;32m    242\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmemory\u001B[38;5;241m.\u001B[39mpush(state, action, skip_reward, next_state, done)\n\u001B[0;32m    244\u001B[0m \u001B[38;5;66;03m# Optimize model\u001B[39;00m\n\u001B[1;32m--> 245\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptimize_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    247\u001B[0m state \u001B[38;5;241m=\u001B[39m next_state\n\u001B[0;32m    248\u001B[0m episode_reward \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m skip_reward\n",
      "Cell \u001B[1;32mIn[1], line 136\u001B[0m, in \u001B[0;36mDQNAgent.optimize_model\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    134\u001B[0m loss\u001B[38;5;241m.\u001B[39mbackward()\n\u001B[0;32m    135\u001B[0m torch\u001B[38;5;241m.\u001B[39mnn\u001B[38;5;241m.\u001B[39mutils\u001B[38;5;241m.\u001B[39mclip_grad_norm_(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpolicy_net\u001B[38;5;241m.\u001B[39mparameters(), \u001B[38;5;241m10\u001B[39m)\n\u001B[1;32m--> 136\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptimizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\optim\\optimizer.py:484\u001B[0m, in \u001B[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    479\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    480\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[0;32m    481\u001B[0m                 \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfunc\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mresult\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    482\u001B[0m             )\n\u001B[1;32m--> 484\u001B[0m out \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    485\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_optimizer_step_code()\n\u001B[0;32m    487\u001B[0m \u001B[38;5;66;03m# call optimizer step post hooks\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\optim\\optimizer.py:89\u001B[0m, in \u001B[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m     87\u001B[0m     torch\u001B[38;5;241m.\u001B[39mset_grad_enabled(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdefaults[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdifferentiable\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n\u001B[0;32m     88\u001B[0m     torch\u001B[38;5;241m.\u001B[39m_dynamo\u001B[38;5;241m.\u001B[39mgraph_break()\n\u001B[1;32m---> 89\u001B[0m     ret \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     90\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m     91\u001B[0m     torch\u001B[38;5;241m.\u001B[39m_dynamo\u001B[38;5;241m.\u001B[39mgraph_break()\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\optim\\adam.py:226\u001B[0m, in \u001B[0;36mAdam.step\u001B[1;34m(self, closure)\u001B[0m\n\u001B[0;32m    214\u001B[0m     beta1, beta2 \u001B[38;5;241m=\u001B[39m group[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbetas\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[0;32m    216\u001B[0m     has_complex \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_init_group(\n\u001B[0;32m    217\u001B[0m         group,\n\u001B[0;32m    218\u001B[0m         params_with_grad,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    223\u001B[0m         state_steps,\n\u001B[0;32m    224\u001B[0m     )\n\u001B[1;32m--> 226\u001B[0m     \u001B[43madam\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    227\u001B[0m \u001B[43m        \u001B[49m\u001B[43mparams_with_grad\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    228\u001B[0m \u001B[43m        \u001B[49m\u001B[43mgrads\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    229\u001B[0m \u001B[43m        \u001B[49m\u001B[43mexp_avgs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    230\u001B[0m \u001B[43m        \u001B[49m\u001B[43mexp_avg_sqs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    231\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmax_exp_avg_sqs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    232\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstate_steps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    233\u001B[0m \u001B[43m        \u001B[49m\u001B[43mamsgrad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mamsgrad\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    234\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhas_complex\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhas_complex\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    235\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbeta1\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbeta1\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    236\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbeta2\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbeta2\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    237\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlr\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mlr\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    238\u001B[0m \u001B[43m        \u001B[49m\u001B[43mweight_decay\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mweight_decay\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    239\u001B[0m \u001B[43m        \u001B[49m\u001B[43meps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43meps\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    240\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmaximize\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmaximize\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    241\u001B[0m \u001B[43m        \u001B[49m\u001B[43mforeach\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mforeach\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    242\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcapturable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcapturable\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    243\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdifferentiable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mdifferentiable\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    244\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfused\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mfused\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    245\u001B[0m \u001B[43m        \u001B[49m\u001B[43mgrad_scale\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mgetattr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mgrad_scale\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    246\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfound_inf\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mgetattr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mfound_inf\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    247\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    249\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m loss\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\optim\\optimizer.py:161\u001B[0m, in \u001B[0;36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    159\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m disabled_func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    160\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 161\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\optim\\adam.py:766\u001B[0m, in \u001B[0;36madam\u001B[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001B[0m\n\u001B[0;32m    763\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    764\u001B[0m     func \u001B[38;5;241m=\u001B[39m _single_tensor_adam\n\u001B[1;32m--> 766\u001B[0m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    767\u001B[0m \u001B[43m    \u001B[49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    768\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgrads\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    769\u001B[0m \u001B[43m    \u001B[49m\u001B[43mexp_avgs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    770\u001B[0m \u001B[43m    \u001B[49m\u001B[43mexp_avg_sqs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    771\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmax_exp_avg_sqs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    772\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstate_steps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    773\u001B[0m \u001B[43m    \u001B[49m\u001B[43mamsgrad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mamsgrad\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    774\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhas_complex\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhas_complex\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    775\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbeta1\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbeta1\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    776\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbeta2\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbeta2\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    777\u001B[0m \u001B[43m    \u001B[49m\u001B[43mlr\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlr\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    778\u001B[0m \u001B[43m    \u001B[49m\u001B[43mweight_decay\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mweight_decay\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    779\u001B[0m \u001B[43m    \u001B[49m\u001B[43meps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43meps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    780\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmaximize\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmaximize\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    781\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcapturable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcapturable\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    782\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdifferentiable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdifferentiable\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    783\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgrad_scale\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgrad_scale\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    784\u001B[0m \u001B[43m    \u001B[49m\u001B[43mfound_inf\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfound_inf\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    785\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\optim\\adam.py:536\u001B[0m, in \u001B[0;36m_multi_tensor_adam\u001B[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001B[0m\n\u001B[0;32m    533\u001B[0m \u001B[38;5;66;03m# Decay the first and second moment running average coefficient\u001B[39;00m\n\u001B[0;32m    534\u001B[0m torch\u001B[38;5;241m.\u001B[39m_foreach_lerp_(device_exp_avgs, device_grads, \u001B[38;5;241m1\u001B[39m \u001B[38;5;241m-\u001B[39m beta1)\n\u001B[1;32m--> 536\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_foreach_mul_\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice_exp_avg_sqs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbeta2\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    537\u001B[0m torch\u001B[38;5;241m.\u001B[39m_foreach_addcmul_(\n\u001B[0;32m    538\u001B[0m     device_exp_avg_sqs, device_grads, device_grads, \u001B[38;5;241m1\u001B[39m \u001B[38;5;241m-\u001B[39m beta2\n\u001B[0;32m    539\u001B[0m )\n\u001B[0;32m    541\u001B[0m \u001B[38;5;66;03m# Delete the local intermediate since it won't be used anymore to save on peak memory\u001B[39;00m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 1
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
