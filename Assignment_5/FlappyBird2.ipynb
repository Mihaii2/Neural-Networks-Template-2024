{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-23T10:18:00.075490Z",
     "start_time": "2024-12-23T10:14:53.950577Z"
    }
   },
   "source": [
    "import gymnasium as gym\n",
    "import flappy_bird_gymnasium\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import random\n",
    "from PIL import Image\n",
    "import os\n",
    "from datetime import datetime\n",
    "import yaml\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_channels=4, n_actions=2):  # Changed from input_channels=1\n",
    "        super(DQN, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(3136, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, n_actions)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        return self.fc_layers(x)\n",
    "        \n",
    "class FrameStack:\n",
    "    def __init__(self, size=4):\n",
    "        self.size = size\n",
    "        self.frames = deque(maxlen=size)\n",
    "    \n",
    "    def push(self, frame):\n",
    "        self.frames.append(frame)\n",
    "        # Randomly save visualizations\n",
    "        if random.random() < 0.01:\n",
    "            self.visualize_stack()\n",
    "    \n",
    "    def get_state(self):\n",
    "        # If we don't have enough frames, duplicate the last frame\n",
    "        while len(self.frames) < self.size:\n",
    "            if len(self.frames) > 0:\n",
    "                self.frames.append(self.frames[-1])\n",
    "            else:\n",
    "                # Create a zero tensor with same shape as expected frame: [1, 1, 84, 84]\n",
    "                zero_frame = torch.zeros(1, 1, 84, 84)\n",
    "                self.frames.append(zero_frame)\n",
    "        \n",
    "        # Stack frames along channel dimension\n",
    "        return torch.cat(list(self.frames), dim=1)  # Will give [1, 4, 84, 84]\n",
    "\n",
    "    def visualize_stack(self):\n",
    "        \"\"\"Save visualization of the current frame stack\"\"\"\n",
    "        if len(self.frames) == 0:\n",
    "            return\n",
    "            \n",
    "        os.makedirs('frame_stacks', exist_ok=True)\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        \n",
    "        # Create a figure with subplots for each frame\n",
    "        import matplotlib.pyplot as plt\n",
    "        fig, axs = plt.subplots(1, self.size, figsize=(20, 5))\n",
    "        \n",
    "        for i, frame in enumerate(self.frames):\n",
    "            # Convert tensor to numpy and remove batch and channel dimensions\n",
    "            frame_np = frame.squeeze().numpy()\n",
    "            axs[i].imshow(frame_np, cmap='gray')\n",
    "            axs[i].axis('off')\n",
    "            axs[i].set_title(f'Frame {i+1}')\n",
    "        \n",
    "        plt.suptitle('Current Frame Stack')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join('frame_stacks', f'frame_stack_{timestamp}.png'))\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity=100000):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "        \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        state, action, reward, next_state, done = zip(*random.sample(self.buffer, batch_size))\n",
    "        return (torch.cat(state), \n",
    "                torch.tensor(action), \n",
    "                torch.tensor(reward), \n",
    "                torch.cat(next_state),\n",
    "                torch.tensor(done))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "def preprocess_image(image):\n",
    "    \"\"\"\n",
    "    Preprocess the image by converting it to grayscale, cropping the bottom, and resizing it.\n",
    "    Returns the image as a PyTorch tensor.\n",
    "    \"\"\"\n",
    "    # Convert to PIL Image for processing\n",
    "    pil_image = Image.fromarray(image)\n",
    "    \n",
    "    # Get image dimensions\n",
    "    width, height = pil_image.size\n",
    "    \n",
    "    # Crop the bottom portion (remove approximately 20% from bottom)\n",
    "    crop_height = int(height * 0.8)  # Keep top 80%\n",
    "    cropped_image = pil_image.crop((0, 0, width, crop_height))\n",
    "    \n",
    "    # Convert to grayscale\n",
    "    grayscale_image = cropped_image.convert(\"L\")\n",
    "    \n",
    "    # Resize the image to a fixed size (84x84)\n",
    "    resized_image = grayscale_image.resize((84, 84))\n",
    "    \n",
    "    # Convert to numpy array and normalize\n",
    "    preprocessed_image = np.array(resized_image) / 255.0\n",
    "    \n",
    "    # Convert to PyTorch tensor and add batch and channel dimensions\n",
    "    tensor_image = torch.FloatTensor(preprocessed_image).unsqueeze(0).unsqueeze(0)\n",
    "    \n",
    "    return tensor_image\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.policy_net = DQN(input_channels=4).to(self.device)  # Changed to 4 input channels\n",
    "        self.target_net = DQN(input_channels=4).to(self.device)  # Changed to 4 input channels\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        self.frame_stack = FrameStack(size=4)  # Add frame stack\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=0.0001)\n",
    "        self.memory = ReplayBuffer()\n",
    "        \n",
    "        self.batch_size = 32\n",
    "        self.gamma = 0.95\n",
    "        self.epsilon_start = 0.3\n",
    "        self.epsilon_end = 0.01\n",
    "        self.epsilon_decay = 1000\n",
    "        self.current_epsilon = self.epsilon_start\n",
    "        self.target_update = 1000\n",
    "        self.frame_skip = 4\n",
    "        \n",
    "        self.steps_done = 0\n",
    "        \n",
    "    def select_action(self, state, training=True):\n",
    "        if self.steps_done % self.epsilon_decay == 0:\n",
    "            decay_amount = 0.001\n",
    "            self.current_epsilon = max(self.epsilon_end, self.current_epsilon - decay_amount)\n",
    "        \n",
    "        if training and random.random() < self.current_epsilon:\n",
    "            return random.randint(0, 1)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            state = state.to(self.device)\n",
    "            q_values = self.policy_net(state)\n",
    "            return q_values.max(1)[1].item()\n",
    "    \n",
    "    def optimize_model(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "        \n",
    "        states, actions, rewards, next_states, dones = self.memory.sample(self.batch_size)\n",
    "        states = states.to(self.device)\n",
    "        actions = actions.to(self.device)\n",
    "        rewards = rewards.to(self.device)\n",
    "        next_states = next_states.to(self.device)\n",
    "        dones = dones.to(self.device)\n",
    "        \n",
    "        current_q_values = self.policy_net(states).gather(1, actions.unsqueeze(1))\n",
    "        next_q_values = self.target_net(next_states).max(1)[0].detach()\n",
    "        expected_q_values = rewards + (1 - dones.float()) * self.gamma * next_q_values\n",
    "        \n",
    "        loss = nn.MSELoss()(current_q_values, expected_q_values.unsqueeze(1))\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.policy_net.parameters(), 10)\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def update_hyperparameters(self):\n",
    "        \"\"\"Update hyperparameters based on the YAML configuration\"\"\"\n",
    "        try:\n",
    "            with open('hyperparameter_updates.yaml', 'r') as file:\n",
    "                config = yaml.safe_load(file)\n",
    "                \n",
    "            if config['status'] == 'Update':\n",
    "                print(\"\\nUpdating hyperparameters...\")\n",
    "                updates = config['updates']\n",
    "                \n",
    "                # Update learning rate\n",
    "                if 'learning_rate' in updates:\n",
    "                    lr_update = updates['learning_rate']\n",
    "                    current_lr = self.optimizer.param_groups[0]['lr']\n",
    "                    \n",
    "                    if lr_update['action'] == 'add':\n",
    "                        new_lr = current_lr + lr_update['value']\n",
    "                    elif lr_update['action'] == 'sub':\n",
    "                        new_lr = current_lr - lr_update['value']\n",
    "                    elif lr_update['action'] == 'mul':\n",
    "                        new_lr = current_lr * lr_update['value']\n",
    "                    \n",
    "                    for param_group in self.optimizer.param_groups:\n",
    "                        param_group['lr'] = new_lr\n",
    "                    print(f\"Learning rate updated: {current_lr:.4f} -> {new_lr:.7f}\")\n",
    "                \n",
    "                # Update current epsilon\n",
    "                if 'epsilon' in updates:\n",
    "                    eps_update = updates['epsilon']\n",
    "                    old_epsilon = self.current_epsilon\n",
    "                    if eps_update['action'] == 'add':\n",
    "                        self.current_epsilon = min(1.0, self.current_epsilon + eps_update['value'])\n",
    "                    elif eps_update['action'] == 'sub':\n",
    "                        self.current_epsilon = max(self.epsilon_end, self.current_epsilon - eps_update['value'])\n",
    "                    print(f\"Current epsilon updated: {old_epsilon:.4f} -> {self.current_epsilon:.4f}\")\n",
    "                \n",
    "                # Update epsilon decay\n",
    "                if 'epsilon_decay' in updates:\n",
    "                    decay_update = updates['epsilon_decay']\n",
    "                    old_decay = self.epsilon_decay\n",
    "                    if decay_update['action'] == 'mul':\n",
    "                        self.epsilon_decay = int(self.epsilon_decay * decay_update['value'])\n",
    "                    print(f\"Epsilon decay updated: {old_decay} -> {self.epsilon_decay}\")\n",
    "                \n",
    "                # Set status to \"Ignore\" after applying updates\n",
    "                config['status'] = 'Ignore'\n",
    "                with open('hyperparameter_updates.yaml', 'w') as file:\n",
    "                    yaml.dump(config, file)\n",
    "                \n",
    "                print(\"Hyperparameter updates complete\\n\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error updating hyperparameters: {str(e)}\")\n",
    "    \n",
    "    def load_model(self, episode_number):\n",
    "        \"\"\"Load a previously saved model and handle architecture changes\"\"\"\n",
    "        model_path = os.path.join('models', f'dqn_episode_{episode_number}.pth')\n",
    "        if os.path.exists(model_path):\n",
    "            checkpoint = torch.load(model_path)\n",
    "            \n",
    "            # Handle the case where we're loading a single-channel model into a four-channel model\n",
    "            old_state_dict = checkpoint['model_state_dict']\n",
    "            new_state_dict = self.policy_net.state_dict()\n",
    "            \n",
    "            # Special handling for the first conv layer\n",
    "            if 'conv_layers.0.weight' in old_state_dict:\n",
    "                old_weights = old_state_dict['conv_layers.0.weight']\n",
    "                if old_weights.size(1) == 1 and new_state_dict['conv_layers.0.weight'].size(1) == 4:\n",
    "                    # Duplicate the single channel weights across all 4 channels\n",
    "                    new_weights = old_weights.repeat(1, 4, 1, 1)\n",
    "                    old_state_dict['conv_layers.0.weight'] = new_weights\n",
    "            \n",
    "            # Load the modified state dict\n",
    "            self.policy_net.load_state_dict(old_state_dict)\n",
    "            self.target_net.load_state_dict(old_state_dict)\n",
    "            self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "            self.steps_done = episode_number * 1000  # Approximate steps based on episode\n",
    "            print(f\"Successfully loaded and adapted model from episode {episode_number}\")\n",
    "            return checkpoint['episode'], checkpoint['reward']\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"No model found at {model_path}\")\n",
    "\n",
    "    def train(self, num_episodes=2000000, start_episode=None, load_existing=True):\n",
    "        \"\"\"Modified train method with optional model loading\"\"\"\n",
    "        os.makedirs('models', exist_ok=True)\n",
    "        os.makedirs('images', exist_ok=True)\n",
    "        \n",
    "        rewards_history = deque(maxlen=100)\n",
    "        best_reward = float('-inf')\n",
    "        \n",
    "        # Load previous model if specified and load_existing is True\n",
    "        if start_episode is not None and load_existing:\n",
    "            try:\n",
    "                episode_num, last_reward = self.load_model(start_episode)\n",
    "                best_reward = last_reward\n",
    "                start_episode = episode_num\n",
    "                print(\"Continuing training from episode\", start_episode)\n",
    "            except FileNotFoundError:\n",
    "                print(f\"No existing model found for episode {start_episode}, starting fresh\")\n",
    "                start_episode = 0\n",
    "        else:\n",
    "            print(\"Starting fresh training run\")\n",
    "            start_episode = 0\n",
    "        \n",
    "        for episode in range(start_episode, num_episodes):\n",
    "            state, _ = self.env.reset()\n",
    "            frame = preprocess_image(self.env.render())\n",
    "            \n",
    "            # Initialize frame stack with first frame\n",
    "            self.frame_stack = FrameStack(size=4)\n",
    "            for _ in range(4):\n",
    "                self.frame_stack.push(frame)\n",
    "            state = self.frame_stack.get_state()\n",
    "            \n",
    "            episode_reward = 0\n",
    "            done = False\n",
    "            \n",
    "            while not done:\n",
    "                action = self.select_action(state)\n",
    "                \n",
    "                # Frame skipping\n",
    "                skip_reward = 0\n",
    "                for _ in range(self.frame_skip):\n",
    "                    next_state, reward, done, truncated, _ = self.env.step(action)\n",
    "                    skip_reward += reward\n",
    "                    if done:\n",
    "                        break\n",
    "                \n",
    "                next_frame = preprocess_image(self.env.render())\n",
    "                self.frame_stack.push(next_frame)\n",
    "                next_state = self.frame_stack.get_state()\n",
    "                \n",
    "                # Store transition in memory\n",
    "                self.memory.push(state, action, skip_reward, next_state, done)\n",
    "                \n",
    "                \n",
    "                # Optimize model\n",
    "                self.optimize_model()\n",
    "                \n",
    "                state = next_state\n",
    "                episode_reward += skip_reward\n",
    "                self.steps_done += 1\n",
    "                \n",
    "                # Update target network\n",
    "                if self.steps_done % self.target_update == 0:\n",
    "                    self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "            \n",
    "            rewards_history.append(episode_reward)\n",
    "            avg_reward = np.mean(rewards_history)\n",
    "            \n",
    "            # Save model periodically\n",
    "            if episode % 1000 == 0:\n",
    "                model_path = os.path.join('models', f'dqn_episode_{episode}.pth')\n",
    "                torch.save({\n",
    "                    'episode': episode,\n",
    "                    'model_state_dict': self.policy_net.state_dict(),\n",
    "                    'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "                    'reward': episode_reward,\n",
    "                }, model_path)\n",
    "                print(f\"Model saved to {model_path}\")\n",
    "            \n",
    "            # Save model when we achieve best episode reward\n",
    "            if episode_reward > best_reward:\n",
    "                best_reward = episode_reward\n",
    "                best_model_path = os.path.join('models', 'dqn_best.pth')\n",
    "                torch.save({\n",
    "                    'episode': episode,\n",
    "                    'model_state_dict': self.policy_net.state_dict(),\n",
    "                    'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "                    'reward': episode_reward,\n",
    "                }, best_model_path)\n",
    "                print(f\"New best reward achieved: {best_reward:.2f}\")\n",
    "            \n",
    "            if (episode + 1) % 100 == 0:\n",
    "                print(f\"Episode {episode + 1}/{num_episodes} - \"\n",
    "                      f\"Reward: {episode_reward:.2f} - \"\n",
    "                      f\"Average Reward (100 ep): {avg_reward:.2f} - \"\n",
    "                      f\"Best Reward: {best_reward:.2f} - \"\n",
    "                      f\"Epsilon: {self.current_epsilon:.7f} - \"\n",
    "                      f\"Learning Rate: {self.optimizer.param_groups[0]['lr']:.7f}\")\n",
    "         \n",
    "            self.update_hyperparameters()\n",
    "\n",
    "def main():\n",
    "    env = gym.make(\"FlappyBird-v0\", render_mode=\"rgb_array\", use_lidar=False)\n",
    "    agent = DQNAgent(env)\n",
    "    agent.train(start_episode=None)\n",
    "    env.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting fresh training run\n",
      "Model saved to models\\dqn_episode_0.pth\n",
      "New best reward achieved: -6.90\n",
      "New best reward achieved: 2.00\n",
      "New best reward achieved: 4.60\n",
      "New best reward achieved: 4.80\n",
      "Episode 100/2000000 - Reward: -9.30 - Average Reward (100 ep): 0.86 - Best Reward: 4.80 - Epsilon: 0.2980000 - Learning Rate: 0.0001000\n",
      "New best reward achieved: 4.90\n",
      "New best reward achieved: 6.00\n",
      "Episode 200/2000000 - Reward: 3.90 - Average Reward (100 ep): 0.49 - Best Reward: 6.00 - Epsilon: 0.2970000 - Learning Rate: 0.0001000\n",
      "Episode 300/2000000 - Reward: 2.80 - Average Reward (100 ep): 1.74 - Best Reward: 6.00 - Epsilon: 0.2960000 - Learning Rate: 0.0001000\n",
      "New best reward achieved: 6.10\n",
      "New best reward achieved: 6.30\n",
      "Episode 400/2000000 - Reward: 3.90 - Average Reward (100 ep): 1.84 - Best Reward: 6.30 - Epsilon: 0.2940000 - Learning Rate: 0.0001000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[8], line 381\u001B[0m\n\u001B[0;32m    378\u001B[0m     env\u001B[38;5;241m.\u001B[39mclose()\n\u001B[0;32m    380\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;18m__name__\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m__main__\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m--> 381\u001B[0m     \u001B[43mmain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[8], line 377\u001B[0m, in \u001B[0;36mmain\u001B[1;34m()\u001B[0m\n\u001B[0;32m    375\u001B[0m env \u001B[38;5;241m=\u001B[39m gym\u001B[38;5;241m.\u001B[39mmake(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFlappyBird-v0\u001B[39m\u001B[38;5;124m\"\u001B[39m, render_mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrgb_array\u001B[39m\u001B[38;5;124m\"\u001B[39m, use_lidar\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[0;32m    376\u001B[0m agent \u001B[38;5;241m=\u001B[39m DQNAgent(env)\n\u001B[1;32m--> 377\u001B[0m \u001B[43magent\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstart_episode\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m    378\u001B[0m env\u001B[38;5;241m.\u001B[39mclose()\n",
      "Cell \u001B[1;32mIn[8], line 328\u001B[0m, in \u001B[0;36mDQNAgent.train\u001B[1;34m(self, num_episodes, start_episode, load_existing)\u001B[0m\n\u001B[0;32m    324\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmemory\u001B[38;5;241m.\u001B[39mpush(state, action, skip_reward, next_state, done)\n\u001B[0;32m    327\u001B[0m \u001B[38;5;66;03m# Optimize model\u001B[39;00m\n\u001B[1;32m--> 328\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptimize_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    330\u001B[0m state \u001B[38;5;241m=\u001B[39m next_state\n\u001B[0;32m    331\u001B[0m episode_reward \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m skip_reward\n",
      "Cell \u001B[1;32mIn[8], line 187\u001B[0m, in \u001B[0;36mDQNAgent.optimize_model\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    184\u001B[0m loss \u001B[38;5;241m=\u001B[39m nn\u001B[38;5;241m.\u001B[39mMSELoss()(current_q_values, expected_q_values\u001B[38;5;241m.\u001B[39munsqueeze(\u001B[38;5;241m1\u001B[39m))\n\u001B[0;32m    186\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[1;32m--> 187\u001B[0m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    188\u001B[0m torch\u001B[38;5;241m.\u001B[39mnn\u001B[38;5;241m.\u001B[39mutils\u001B[38;5;241m.\u001B[39mclip_grad_norm_(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpolicy_net\u001B[38;5;241m.\u001B[39mparameters(), \u001B[38;5;241m10\u001B[39m)\n\u001B[0;32m    189\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptimizer\u001B[38;5;241m.\u001B[39mstep()\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\_tensor.py:521\u001B[0m, in \u001B[0;36mTensor.backward\u001B[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[0;32m    511\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    512\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[0;32m    513\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[0;32m    514\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    519\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[0;32m    520\u001B[0m     )\n\u001B[1;32m--> 521\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    522\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[0;32m    523\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\autograd\\__init__.py:289\u001B[0m, in \u001B[0;36mbackward\u001B[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[0;32m    284\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[0;32m    286\u001B[0m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[0;32m    287\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[0;32m    288\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[1;32m--> 289\u001B[0m \u001B[43m_engine_run_backward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    290\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    291\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    292\u001B[0m \u001B[43m    \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    293\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    294\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    295\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    296\u001B[0m \u001B[43m    \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    297\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\autograd\\graph.py:769\u001B[0m, in \u001B[0;36m_engine_run_backward\u001B[1;34m(t_outputs, *args, **kwargs)\u001B[0m\n\u001B[0;32m    767\u001B[0m     unregister_hooks \u001B[38;5;241m=\u001B[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001B[0;32m    768\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 769\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[0;32m    770\u001B[0m \u001B[43m        \u001B[49m\u001B[43mt_outputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\n\u001B[0;32m    771\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[0;32m    772\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m    773\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m attach_logging_hooks:\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 8
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
