{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-23T23:03:41.168884Z",
     "start_time": "2024-12-23T23:02:18.394385Z"
    }
   },
   "source": [
    "import gymnasium as gym\n",
    "import flappy_bird_gymnasium\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import random\n",
    "from PIL import Image\n",
    "import os\n",
    "from datetime import datetime\n",
    "import yaml\n",
    "import gc\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_channels=4, n_actions=2):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, 32, kernel_size=8, stride=4),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.LeakyReLU(0.01)\n",
    "        )\n",
    "\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(3136, 512),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.Linear(512, n_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc_layers(x)\n",
    "\n",
    "class FrameStack:\n",
    "    def __init__(self, size=4):\n",
    "        self.size = size\n",
    "        self.frames = deque(maxlen=size)\n",
    "\n",
    "    def push(self, frame):\n",
    "        # Ensure frame is detached from computation graph and on CPU\n",
    "        if torch.is_tensor(frame):\n",
    "            frame = frame.cpu().detach()\n",
    "        self.frames.append(frame)\n",
    "\n",
    "    def get_state(self):\n",
    "        while len(self.frames) < self.size:\n",
    "            if len(self.frames) > 0:\n",
    "                self.frames.append(self.frames[-1])\n",
    "            else:\n",
    "                zero_frame = torch.zeros(1, 1, 84, 84)\n",
    "                self.frames.append(zero_frame)\n",
    "\n",
    "        # Stack frames and detach to prevent memory leaks\n",
    "        stacked = torch.cat(list(self.frames), dim=1)\n",
    "        return stacked.detach()\n",
    "\n",
    "    def clear(self):\n",
    "        self.frames.clear()\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity=50000): \n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        # Move tensors to CPU and detach from computation graph\n",
    "        if torch.is_tensor(state):\n",
    "            state = state.cpu().detach()\n",
    "        if torch.is_tensor(next_state):\n",
    "            next_state = next_state.cpu().detach()\n",
    "\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        samples = random.sample(self.buffer, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*samples)\n",
    "\n",
    "        # Stack all states efficiently\n",
    "        states = torch.cat(states)\n",
    "        next_states = torch.cat(next_states)\n",
    "        actions = torch.tensor(actions)\n",
    "        rewards = torch.tensor(rewards)\n",
    "        dones = torch.tensor(dones)\n",
    "\n",
    "        return states, actions, rewards, next_states, dones\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "    def clear(self):\n",
    "        self.buffer.clear()\n",
    "\n",
    "def preprocess_image(image):\n",
    "    pil_image = Image.fromarray(image)\n",
    "    width, height = pil_image.size\n",
    "    crop_height = int(height * 0.8)\n",
    "    cropped_image = pil_image.crop((0, 0, width, crop_height))\n",
    "    grayscale_image = cropped_image.convert(\"L\")\n",
    "    resized_image = grayscale_image.resize((84, 84))\n",
    "    preprocessed_image = np.array(resized_image) / 255.0\n",
    "    tensor_image = torch.FloatTensor(preprocessed_image).unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "    # Clean up PIL Images\n",
    "    del pil_image\n",
    "    del cropped_image\n",
    "    del grayscale_image\n",
    "    del resized_image\n",
    "\n",
    "    return tensor_image\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.policy_net = DQN(input_channels=4).to(self.device)\n",
    "        self.target_net = DQN(input_channels=4).to(self.device)\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        self.frame_stack = FrameStack(size=4)\n",
    "\n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=0.00025)\n",
    "        self.memory = ReplayBuffer(capacity=50000)\n",
    "\n",
    "        self.batch_size = 32\n",
    "        self.gamma = 0.99\n",
    "        self.epsilon_start = 0.2\n",
    "        self.epsilon_end = 0.025\n",
    "        self.epsilon_decay = 100\n",
    "        self.current_epsilon = self.epsilon_start\n",
    "        self.target_update = 1000\n",
    "        self.frame_skip = 0\n",
    "\n",
    "        self.steps_done = 0\n",
    "\n",
    "    def select_action(self, state, training=True):\n",
    "        if self.steps_done % self.epsilon_decay == 0:\n",
    "            decay_amount = 0.001\n",
    "            self.current_epsilon = max(self.epsilon_end, self.current_epsilon - decay_amount)\n",
    "\n",
    "        if training and random.random() < self.current_epsilon:\n",
    "            return random.randint(0, 1)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            state = state.to(self.device)\n",
    "            q_values = self.policy_net(state)\n",
    "            action = q_values.max(1)[1].item()\n",
    "            # Clean up\n",
    "            del q_values\n",
    "            return action\n",
    "\n",
    "    def optimize_model(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "\n",
    "        # Sample and move to device\n",
    "        states, actions, rewards, next_states, dones = self.memory.sample(self.batch_size)\n",
    "        states = states.to(self.device)\n",
    "        actions = actions.to(self.device)\n",
    "        rewards = rewards.to(self.device)\n",
    "        next_states = next_states.to(self.device)\n",
    "        dones = dones.to(self.device)\n",
    "\n",
    "        # Compute loss\n",
    "        with torch.no_grad():\n",
    "            next_actions = self.policy_net(next_states).max(1)[1].unsqueeze(1)\n",
    "            next_q_values = self.target_net(next_states).gather(1, next_actions)\n",
    "            expected_q_values = rewards.unsqueeze(1) + (1 - dones.float().unsqueeze(1)) * self.gamma * next_q_values\n",
    "\n",
    "        current_q_values = self.policy_net(states).gather(1, actions.unsqueeze(1))\n",
    "        loss = nn.MSELoss()(current_q_values, expected_q_values)\n",
    "\n",
    "        # Optimize\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.policy_net.parameters(), 10)\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # Clean up\n",
    "        del states, actions, rewards, next_states, dones\n",
    "        del next_actions, next_q_values, expected_q_values, current_q_values\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    def update_hyperparameters(self):\n",
    "        \"\"\"Update hyperparameters based on the YAML configuration\"\"\"\n",
    "        try:\n",
    "            with open('hyperparameter_updates.yaml', 'r') as file:\n",
    "                config = yaml.safe_load(file)\n",
    "\n",
    "            if config['status'] == 'Update':\n",
    "                print(\"\\nUpdating hyperparameters...\")\n",
    "                updates = config['updates']\n",
    "\n",
    "                # Update learning rate\n",
    "                if 'learning_rate' in updates:\n",
    "                    lr_update = updates['learning_rate']\n",
    "                    current_lr = self.optimizer.param_groups[0]['lr']\n",
    "\n",
    "                    if lr_update['action'] == 'add':\n",
    "                        new_lr = current_lr + lr_update['value']\n",
    "                    elif lr_update['action'] == 'sub':\n",
    "                        new_lr = current_lr - lr_update['value']\n",
    "                    elif lr_update['action'] == 'mul':\n",
    "                        new_lr = current_lr * lr_update['value']\n",
    "\n",
    "                    for param_group in self.optimizer.param_groups:\n",
    "                        param_group['lr'] = new_lr\n",
    "                    print(f\"Learning rate updated: {current_lr:.4f} -> {new_lr:.7f}\")\n",
    "\n",
    "                # Update current epsilon\n",
    "                if 'epsilon' in updates:\n",
    "                    eps_update = updates['epsilon']\n",
    "                    old_epsilon = self.current_epsilon\n",
    "                    if eps_update['action'] == 'add':\n",
    "                        self.current_epsilon = min(1.0, self.current_epsilon + eps_update['value'])\n",
    "                    elif eps_update['action'] == 'sub':\n",
    "                        self.current_epsilon = max(self.epsilon_end, self.current_epsilon - eps_update['value'])\n",
    "                    print(f\"Current epsilon updated: {old_epsilon:.4f} -> {self.current_epsilon:.4f}\")\n",
    "\n",
    "                # Update epsilon decay\n",
    "                if 'epsilon_decay' in updates:\n",
    "                    decay_update = updates['epsilon_decay']\n",
    "                    old_decay = self.epsilon_decay\n",
    "                    if decay_update['action'] == 'mul':\n",
    "                        self.epsilon_decay = int(self.epsilon_decay * decay_update['value'])\n",
    "                    print(f\"Epsilon decay updated: {old_decay} -> {self.epsilon_decay}\")\n",
    "\n",
    "                # Set status to \"Ignore\" after applying updates\n",
    "                config['status'] = 'Ignore'\n",
    "                with open('hyperparameter_updates.yaml', 'w') as file:\n",
    "                    yaml.dump(config, file)\n",
    "\n",
    "                print(\"Hyperparameter updates complete\\n\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error updating hyperparameters: {str(e)}\")\n",
    "\n",
    "    def load_model(self, episode_number):\n",
    "        \"\"\"Load a previously saved model and handle architecture changes\"\"\"\n",
    "        model_path = os.path.join('models', f'dqn_episode_{episode_number}.pth')\n",
    "        if os.path.exists(model_path):\n",
    "            checkpoint = torch.load(model_path)\n",
    "\n",
    "            # Handle the case where we're loading a single-channel model into a four-channel model\n",
    "            old_state_dict = checkpoint['model_state_dict']\n",
    "            new_state_dict = self.policy_net.state_dict()\n",
    "\n",
    "            # Special handling for the first conv layer\n",
    "            if 'conv_layers.0.weight' in old_state_dict:\n",
    "                old_weights = old_state_dict['conv_layers.0.weight']\n",
    "                if old_weights.size(1) == 1 and new_state_dict['conv_layers.0.weight'].size(1) == 4:\n",
    "                    # Duplicate the single channel weights across all 4 channels\n",
    "                    new_weights = old_weights.repeat(1, 4, 1, 1)\n",
    "                    old_state_dict['conv_layers.0.weight'] = new_weights\n",
    "\n",
    "            # Load the modified state dict\n",
    "            self.policy_net.load_state_dict(old_state_dict)\n",
    "            self.target_net.load_state_dict(old_state_dict)\n",
    "            self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "            self.steps_done = episode_number * 1000  # Approximate steps based on episode\n",
    "            print(f\"Successfully loaded and adapted model from episode {episode_number}\")\n",
    "            return checkpoint['episode'], checkpoint['reward']\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"No model found at {model_path}\")\n",
    "\n",
    "    def clear_memory(self):\n",
    "        self.memory.clear()\n",
    "        self.frame_stack.clear()\n",
    "        gc.collect()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    def train(self, num_episodes=2000000, start_episode=None, load_existing=True):\n",
    "        os.makedirs('models', exist_ok=True)\n",
    "        os.makedirs('images', exist_ok=True)\n",
    "\n",
    "        rewards_history = deque(maxlen=100)\n",
    "        best_reward = float('-inf')\n",
    "\n",
    "        if start_episode is not None and load_existing:\n",
    "            try:\n",
    "                episode_num, last_reward = self.load_model(start_episode)\n",
    "                best_reward = last_reward\n",
    "                start_episode = episode_num\n",
    "                print(\"Continuing training from episode\", start_episode)\n",
    "            except FileNotFoundError:\n",
    "                print(f\"No existing model found for episode {start_episode}, starting fresh\")\n",
    "                start_episode = 0\n",
    "        else:\n",
    "            start_episode = 0\n",
    "\n",
    "        try:\n",
    "            for episode in range(start_episode, num_episodes):\n",
    "                state, _ = self.env.reset()\n",
    "                frame = preprocess_image(self.env.render())\n",
    "\n",
    "                self.frame_stack = FrameStack(size=4)\n",
    "                for _ in range(4):\n",
    "                    self.frame_stack.push(frame)\n",
    "                state = self.frame_stack.get_state()\n",
    "\n",
    "                episode_reward = 0\n",
    "                done = False\n",
    "\n",
    "                while not done:\n",
    "                    action = self.select_action(state)\n",
    "\n",
    "                    skip_reward = 0\n",
    "                    for _ in range(self.frame_skip):\n",
    "                        next_state, reward, done, truncated, _ = self.env.step(action)\n",
    "                        skip_reward += reward\n",
    "                        if done:\n",
    "                            break\n",
    "\n",
    "                    next_frame = preprocess_image(self.env.render())\n",
    "                    self.frame_stack.push(next_frame)\n",
    "                    next_state = self.frame_stack.get_state()\n",
    "\n",
    "                    self.memory.push(state, action, skip_reward, next_state, done)\n",
    "                    self.optimize_model()\n",
    "\n",
    "                    state = next_state\n",
    "                    episode_reward += skip_reward\n",
    "                    self.steps_done += 1\n",
    "\n",
    "                    if self.steps_done % self.target_update == 0:\n",
    "                        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "\n",
    "                rewards_history.append(episode_reward)\n",
    "                avg_reward = np.mean(rewards_history)\n",
    "\n",
    "                # Periodic cleanup\n",
    "                if episode % 100 == 0:\n",
    "                    gc.collect()\n",
    "                    if torch.cuda.is_available():\n",
    "                        torch.cuda.empty_cache()\n",
    "\n",
    "                # Save model periodically\n",
    "                if episode % 100 == 0:\n",
    "                    self.clear_memory()  # Clear memory before saving\n",
    "                    model_path = os.path.join('models', f'dqn_episode_{episode}.pth')\n",
    "                    torch.save({\n",
    "                        'episode': episode,\n",
    "                        'model_state_dict': self.policy_net.state_dict(),\n",
    "                        'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "                        'reward': episode_reward,\n",
    "                    }, model_path)\n",
    "                    print(f\"Model saved at episode {episode}\")\n",
    "\n",
    "                if episode_reward > best_reward:\n",
    "                    best_reward = episode_reward\n",
    "                    best_model_path = os.path.join('models', 'dqn_best.pth')\n",
    "                    torch.save({\n",
    "                        'episode': episode,\n",
    "                        'model_state_dict': self.policy_net.state_dict(),\n",
    "                        'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "                        'reward': episode_reward,\n",
    "                    }, best_model_path)\n",
    "                    print(f\"New best model saved. Episode {episode} with reward {episode_reward:.2f}\")\n",
    "\n",
    "                if (episode + 1) % 100 == 0:\n",
    "                    print(f\"Episode {episode + 1}/{num_episodes} - \"\n",
    "                          f\"Reward: {episode_reward:.2f} - \"\n",
    "                          f\"Average Reward (100 ep): {avg_reward:.2f} - \"\n",
    "                          f\"Best Reward: {best_reward:.2f} - \"\n",
    "                          f\"Epsilon: {self.current_epsilon:.7f} - \"\n",
    "                          f\"Learning Rate: {self.optimizer.param_groups[0]['lr']:.7f}\")\n",
    "\n",
    "                self.update_hyperparameters()\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\nTraining interrupted. Cleaning up...\")\n",
    "            self.clear_memory()\n",
    "\n",
    "        finally:\n",
    "            self.clear_memory()\n",
    "\n",
    "def main():\n",
    "    env = gym.make(\"FlappyBird-v0\", render_mode=\"rgb_array\", use_lidar=False)\n",
    "    agent = DQNAgent(env)\n",
    "\n",
    "    try:\n",
    "        agent.train(start_episode=None)\n",
    "    finally:\n",
    "        env.close()\n",
    "        gc.collect()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training interrupted. Cleaning up...\n"
     ]
    }
   ],
   "execution_count": 10
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
